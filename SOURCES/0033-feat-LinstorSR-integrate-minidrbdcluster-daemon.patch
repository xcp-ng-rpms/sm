From 6845d1441c22ca5e5145af82669416a48a5a813e Mon Sep 17 00:00:00 2001
From: Wescoeur <ronan.abhamon@vates.fr>
Date: Wed, 20 Jan 2021 18:04:26 +0100
Subject: [PATCH 033/177] feat(LinstorSR): integrate minidrbdcluster daemon

Now, we can:
- Start a controller on any node
- Share the LINSTOR volume list using a specific volume "xcp-persistent-database"
- Use the HA with "xcp-persistent-ha-statefile" and "xcp-persistent-redo-log" volumes
- Create the nodes automatically during SR creation

Signed-off-by: Ronan Abhamon <ronan.abhamon@vates.fr>
---
 Makefile                                      |  14 +
 drivers/LinstorSR.py                          | 269 ++++---
 drivers/cleanup.py                            |  12 +-
 drivers/linstor-manager                       | 111 ++-
 drivers/linstorjournaler.py                   |  40 +-
 drivers/linstorvolumemanager.py               | 691 +++++++++++++++---
 drivers/tapdisk-pause                         |   8 +-
 drivers/util.py                               |  21 -
 etc/minidrbdcluster.ini                       |  14 +
 .../linstor-satellite.service.d/override.conf |   5 +
 etc/systemd/system/var-lib-linstor.mount      |   6 +
 linstor/linstor-monitord.c                    |  15 -
 scripts/minidrbdcluster                       | 171 +++++
 systemd/minidrbdcluster.service               |  18 +
 14 files changed, 1124 insertions(+), 271 deletions(-)
 create mode 100644 etc/minidrbdcluster.ini
 create mode 100644 etc/systemd/system/linstor-satellite.service.d/override.conf
 create mode 100644 etc/systemd/system/var-lib-linstor.mount
 create mode 100755 scripts/minidrbdcluster
 create mode 100644 systemd/minidrbdcluster.service

diff --git a/Makefile b/Makefile
index 284b9a39..43dd5692 100755
--- a/Makefile
+++ b/Makefile
@@ -92,6 +92,7 @@ PLUGIN_SCRIPT_DEST := /etc/xapi.d/plugins/
 LIBEXEC := /opt/xensource/libexec/
 UDEV_RULES_DIR := /etc/udev/rules.d/
 UDEV_SCRIPTS_DIR := /etc/udev/scripts/
+SYSTEMD_CONF_DIR := /etc/systemd/system/
 SYSTEMD_SERVICE_DIR := /usr/lib/systemd/system/
 INIT_DIR := /etc/rc.d/init.d/
 MPATH_CONF_DIR := /etc/multipath.xenserver/
@@ -99,6 +100,7 @@ MPATH_CUSTOM_CONF_DIR := /etc/multipath/conf.d/
 MODPROBE_DIR := /etc/modprobe.d/
 EXTENSION_SCRIPT_DEST := /etc/xapi.d/extensions/
 LOGROTATE_DIR := /etc/logrotate.d/
+MINI_DRBD_CLUSTER_CONF_DIR := /etc/
 
 SM_STAGING := $(DESTDIR)
 SM_STAMP := $(MY_OBJ_DIR)/.staging_stamp
@@ -147,11 +149,14 @@ install: precheck
 	mkdir -p $(SM_STAGING)$(UDEV_RULES_DIR)
 	mkdir -p $(SM_STAGING)$(UDEV_SCRIPTS_DIR)
 	mkdir -p $(SM_STAGING)$(INIT_DIR)
+	mkdir -p $(SM_STAGING)$(SYSTEMD_CONF_DIR)
+	mkdir -p $(SM_STAGING)$(SYSTEMD_CONF_DIR)/linstor-satellite.service.d
 	mkdir -p $(SM_STAGING)$(SYSTEMD_SERVICE_DIR)
 	mkdir -p $(SM_STAGING)$(MPATH_CONF_DIR)
 	mkdir -p $(SM_STAGING)$(MPATH_CUSTOM_CONF_DIR)
 	mkdir -p $(SM_STAGING)$(MODPROBE_DIR)
 	mkdir -p $(SM_STAGING)$(LOGROTATE_DIR)
+	mkdir -p $(SM_STAGING)$(MINI_DRBD_CLUSTER_CONF_DIR)
 	mkdir -p $(SM_STAGING)$(DEBUG_DEST)
 	mkdir -p $(SM_STAGING)$(BIN_DEST)
 	mkdir -p $(SM_STAGING)$(MASTER_SCRIPT_DEST)
@@ -175,6 +180,12 @@ install: precheck
 	  $(SM_STAGING)/$(SM_DEST)
 	install -m 644 etc/logrotate.d/$(SMLOG_CONF) \
 	  $(SM_STAGING)/$(LOGROTATE_DIR)
+	install -m 644 etc/systemd/system/linstor-satellite.service.d/override.conf \
+	  $(SM_STAGING)/$(SYSTEMD_CONF_DIR)/linstor-satellite.service.d/
+	install -m 644 etc/systemd/system/var-lib-linstor.mount \
+	  $(SM_STAGING)/$(SYSTEMD_CONF_DIR)
+	install -m 644 etc/minidrbdcluster.ini \
+	  $(SM_STAGING)/$(MINI_DRBD_CLUSTER_CONF_DIR)
 	install -m 644 etc/make-dummy-sr.service \
 	  $(SM_STAGING)/$(SYSTEMD_SERVICE_DIR)
 	install -m 644 systemd/xs-sm.service \
@@ -193,6 +204,8 @@ install: precheck
 	  $(SM_STAGING)/$(SYSTEMD_SERVICE_DIR)
 	install -m 644 systemd/linstor-monitor.service \
 	  $(SM_STAGING)/$(SYSTEMD_SERVICE_DIR)
+	install -m 644 systemd/minidrbdcluster.service \
+	  $(SM_STAGING)/$(SYSTEMD_SERVICE_DIR)
 	for i in $(UDEV_RULES); do \
 	  install -m 644 udev/$$i.rules \
 	    $(SM_STAGING)$(UDEV_RULES_DIR); done
@@ -242,6 +255,7 @@ install: precheck
 	install -m 755 scripts/xe-getlunidentifier $(SM_STAGING)$(BIN_DEST)
 	install -m 755 scripts/make-dummy-sr $(SM_STAGING)$(LIBEXEC)
 	install -m 755 scripts/storage-init $(SM_STAGING)$(LIBEXEC)
+	install -m 755 scripts/minidrbdcluster $(SM_STAGING)$(LIBEXEC)
 
 .PHONY: clean
 clean:
diff --git a/drivers/LinstorSR.py b/drivers/LinstorSR.py
index 2df2d681..9650d712 100755
--- a/drivers/LinstorSR.py
+++ b/drivers/LinstorSR.py
@@ -19,8 +19,11 @@ from constants import CBTLOG_TAG
 try:
     from linstorjournaler import LinstorJournaler
     from linstorvhdutil import LinstorVhdUtil
-    from linstorvolumemanager \
-        import LinstorVolumeManager, LinstorVolumeManagerError
+    from linstorvolumemanager import get_controller_uri
+    from linstorvolumemanager import get_controller_node_name
+    from linstorvolumemanager import LinstorVolumeManager
+    from linstorvolumemanager import LinstorVolumeManagerError
+
     LINSTOR_AVAILABLE = True
 except ImportError:
     LINSTOR_AVAILABLE = False
@@ -310,7 +313,7 @@ class LinstorSR(SR.SR):
         self.lock = Lock(vhdutil.LOCK_TYPE_SR, self.uuid)
         self.sr_vditype = SR.DEFAULT_TAP
 
-        self._hosts = self.dconf['hosts'].split(',')
+        self._hosts = list(set(self.dconf['hosts'].split(',')))
         self._redundancy = int(self.dconf['redundancy'] or 1)
         self._linstor = None  # Ensure that LINSTOR attribute exists.
         self._journaler = None
@@ -320,7 +323,6 @@ class LinstorSR(SR.SR):
             self._is_master = True
         self._group_name = self.dconf['group-name']
 
-        self._master_uri = None
         self._vdi_shared_time = 0
 
         self._initialized = False
@@ -340,24 +342,18 @@ class LinstorSR(SR.SR):
                 if self.srcmd.cmd == 'vdi_attach_from_config':
                     # We must have a valid LINSTOR instance here without using
                     # the XAPI.
-                    self._master_uri = 'linstor://{}'.format(
-                        util.get_master_address()
-                    )
+                    controller_uri = get_controller_uri()
                     self._journaler = LinstorJournaler(
-                        self._master_uri, self._group_name, logger=util.SMlog
+                        controller_uri, self._group_name, logger=util.SMlog
                     )
 
                     self._linstor = LinstorVolumeManager(
-                        self._master_uri,
+                        controller_uri,
                         self._group_name,
                         logger=util.SMlog
                     )
                 return method(self, *args, **kwargs)
 
-            self._master_uri = 'linstor://{}'.format(
-                util.get_master_rec(self.session)['address']
-            )
-
             if not self._is_master:
                 if self.cmd in [
                     'sr_create', 'sr_delete', 'sr_update', 'sr_probe',
@@ -376,43 +372,31 @@ class LinstorSR(SR.SR):
                     self._shared_lock_vdi(self.srcmd.params['vdi_uuid'])
                     self._vdi_shared_time = time.time()
 
-            self._journaler = LinstorJournaler(
-                self._master_uri, self._group_name, logger=util.SMlog
-            )
+            if self.srcmd.cmd != 'sr_create' and self.srcmd.cmd != 'sr_detach':
+                try:
+                    controller_uri = get_controller_uri()
 
-            # Ensure ports are opened and LINSTOR controller/satellite
-            # are activated.
-            if self.srcmd.cmd == 'sr_create':
-                # TODO: Disable if necessary
-                self._enable_linstor_on_all_hosts(status=True)
+                    self._journaler = LinstorJournaler(
+                        controller_uri, self._group_name, logger=util.SMlog
+                    )
 
-            try:
-                # Try to open SR if exists.
-                # We can repair only if we are on the master AND if
-                # we are trying to execute an exclusive operation.
-                # Otherwise we could try to delete a VDI being created or
-                # during a snapshot. An exclusive op is the guarantee that the
-                # SR is locked.
-                self._linstor = LinstorVolumeManager(
-                    self._master_uri,
-                    self._group_name,
-                    repair=(
-                        self._is_master and
-                        self.srcmd.cmd in self.ops_exclusive
-                    ),
-                    logger=util.SMlog
-                )
-                self._vhdutil = LinstorVhdUtil(self.session, self._linstor)
-            except Exception as e:
-                if self.srcmd.cmd == 'sr_create' or \
-                        self.srcmd.cmd == 'sr_detach':
-                    # Ignore exception in this specific case: sr_create.
-                    # At this moment the LinstorVolumeManager cannot be
-                    # instantiated. Concerning the sr_detach command, we must
-                    # ignore LINSTOR exceptions (if the volume group doesn't
-                    # exist for example after a bad user action).
-                    pass
-                else:
+                    # Try to open SR if exists.
+                    # We can repair only if we are on the master AND if
+                    # we are trying to execute an exclusive operation.
+                    # Otherwise we could try to delete a VDI being created or
+                    # during a snapshot. An exclusive op is the guarantee that
+                    # the SR is locked.
+                    self._linstor = LinstorVolumeManager(
+                        controller_uri,
+                        self._group_name,
+                        repair=(
+                            self._is_master and
+                            self.srcmd.cmd in self.ops_exclusive
+                        ),
+                        logger=util.SMlog
+                    )
+                    self._vhdutil = LinstorVhdUtil(self.session, self._linstor)
+                except Exception as e:
                     raise xs_errors.XenError('SRUnavailable', opterr=str(e))
 
             if self._linstor:
@@ -507,13 +491,44 @@ class LinstorSR(SR.SR):
                         opterr='group name must be unique'
                     )
 
+        if srs:
+            raise xs_errors.XenError(
+                'LinstorSRCreate',
+                opterr='LINSTOR SR must be unique in a pool'
+            )
+
+        online_hosts = util.get_online_hosts(self.session)
+        if len(online_hosts) < len(self._hosts):
+            raise xs_errors.XenError(
+                'LinstorSRCreate',
+                opterr='Not enough online hosts'
+            )
+
+        ips = {}
+        for host in online_hosts:
+            record = self.session.xenapi.host.get_record(host)
+            hostname = record['hostname']
+            if hostname in self._hosts:
+                ips[hostname] = record['address']
+
+        if len(ips) != len(self._hosts):
+            raise xs_errors.XenError(
+                'LinstorSRCreate',
+                opterr='Not enough online hosts'
+            )
+
+        # Ensure ports are opened and LINSTOR satellites
+        # are activated. In the same time the minidrbdcluster instances
+        # must be stopped.
+        self._prepare_sr_on_all_hosts(enabled=True)
+
         # Create SR.
         # Throw if the SR already exists.
         try:
             self._linstor = LinstorVolumeManager.create_sr(
-                self._master_uri,
                 self._group_name,
                 self._hosts,
+                ips,
                 self._redundancy,
                 thin_provisioning=self._provisioning == 'thin',
                 logger=util.SMlog
@@ -523,30 +538,79 @@ class LinstorSR(SR.SR):
             util.SMlog('Failed to create LINSTOR SR: {}'.format(e))
             raise xs_errors.XenError('LinstorSRCreate', opterr=str(e))
 
+        try:
+            util.SMlog(
+                "Finishing SR creation, enable minidrbdcluster on all hosts..."
+            )
+            self._update_minidrbdcluster_on_all_hosts(enabled=True)
+        except Exception as e:
+            try:
+                self._linstor.destroy()
+            except Exception as e2:
+                util.SMlog(
+                    'Failed to destroy LINSTOR SR after creation fail: {}'
+                    .format(e2)
+                )
+            raise e
+
     @_locked_load
     def delete(self, uuid):
         util.SMlog('LinstorSR.delete for {}'.format(self.uuid))
         cleanup.gc_force(self.session, self.uuid)
 
-        if self.vdis:
+        if self.vdis or self._linstor._volumes:
             raise xs_errors.XenError('SRNotEmpty')
 
-        try:
-            # TODO: Use specific exceptions. If the LINSTOR group doesn't
-            # exist, we can remove it without problem.
+        node_name = get_controller_node_name()
+        if not node_name:
+            raise xs_errors.XenError(
+                'LinstorSRDelete',
+                opterr='Cannot get controller node name'
+            )
 
-            # TODO: Maybe remove all volumes unused by the SMAPI.
-            # We must ensure it's a safe idea...
+        host = None
+        if node_name == 'localhost':
+            host = util.get_this_host_ref(self.session)
+        else:
+            for slave in util.get_all_slaves(self.session):
+                r_name = self.session.xenapi.host.get_record(slave)['hostname']
+                if r_name == node_name:
+                    host = slave
+                    break
 
-            self._linstor.destroy()
-            Lock.cleanupAll(self.uuid)
+        if not host:
+            raise xs_errors.XenError(
+                'LinstorSRDelete',
+                opterr='Failed to find host with hostname: {}'.format(
+                    node_name
+                )
+            )
+
+        try:
+            self._update_minidrbdcluster_on_all_hosts(enabled=False)
+
+            args = {
+                'groupName': self._group_name,
+            }
+            self._exec_manager_command(
+                host, 'destroy', args, 'LinstorSRDelete'
+            )
         except Exception as e:
+            try:
+                self._update_minidrbdcluster_on_all_hosts(enabled=True)
+            except Exception as e2:
+                util.SMlog(
+                    'Failed to restart minidrbdcluster after destroy fail: {}'
+                    .format(e2)
+                )
             util.SMlog('Failed to delete LINSTOR SR: {}'.format(e))
             raise xs_errors.XenError(
                 'LinstorSRDelete',
                 opterr=str(e)
             )
 
+        Lock.cleanupAll(self.uuid)
+
     @_locked_load
     def update(self, uuid):
         util.SMlog('LinstorSR.update for {}'.format(self.uuid))
@@ -626,10 +690,9 @@ class LinstorSR(SR.SR):
     # --------------------------------------------------------------------------
 
     def _shared_lock_vdi(self, vdi_uuid, locked=True):
-        pools = self.session.xenapi.pool.get_all()
-        master = self.session.xenapi.pool.get_master(pools[0])
+        master = util.get_master_ref(self.session)
 
-        method = 'lockVdi'
+        command = 'lockVdi'
         args = {
             'groupName': self._group_name,
             'srUuid': self.uuid,
@@ -654,48 +717,56 @@ class LinstorSR(SR.SR):
                 )
                 return
 
-        ret = self.session.xenapi.host.call_plugin(
-            master, self.MANAGER_PLUGIN, method, args
-        )
-        util.SMlog(
-            'call-plugin ({} with {}) returned: {}'
-            .format(method, args, ret)
-        )
-        if ret == 'False':
-            raise xs_errors.XenError(
-                'VDIUnavailable',
-                opterr='Plugin {} failed'.format(self.MANAGER_PLUGIN)
-            )
+        self._exec_manager_command(master, command, args, 'VDIUnavailable')
 
     # --------------------------------------------------------------------------
     # Network.
     # --------------------------------------------------------------------------
 
-    def _enable_linstor(self, host, status):
-        method = 'enable'
-        args = {'enabled': str(bool(status))}
-
+    def _exec_manager_command(self, host, command, args, error):
         ret = self.session.xenapi.host.call_plugin(
-            host, self.MANAGER_PLUGIN, method, args
+            host, self.MANAGER_PLUGIN, command, args
         )
         util.SMlog(
-            'call-plugin ({} with {}) returned: {}'.format(method, args, ret)
+            'call-plugin ({}:{} with {}) returned: {}'.format(
+                self.MANAGER_PLUGIN, command, args, ret
+            )
         )
         if ret == 'False':
             raise xs_errors.XenError(
-                'SRUnavailable',
+                error,
                 opterr='Plugin {} failed'.format(self.MANAGER_PLUGIN)
             )
 
-    def _enable_linstor_on_master(self, status):
-        pools = self.session.xenapi.pool.get_all()
-        master = self.session.xenapi.pool.get_master(pools[0])
-        self._enable_linstor(master, status)
+    def _prepare_sr(self, host, enabled):
+        self._exec_manager_command(
+            host,
+            'prepareSr' if enabled else 'releaseSr',
+            {},
+            'SRUnavailable'
+        )
+
+    def _prepare_sr_on_all_hosts(self, enabled):
+        master = util.get_master_ref(self.session)
+        self._prepare_sr(master, enabled)
 
-    def _enable_linstor_on_all_hosts(self, status):
-        self._enable_linstor_on_master(status)
         for slave in util.get_all_slaves(self.session):
-            self._enable_linstor(slave, status)
+            self._prepare_sr(slave, enabled)
+
+    def _update_minidrbdcluster(self, host, enabled):
+        self._exec_manager_command(
+            host,
+            'updateMinidrbdcluster',
+            {'enabled': str(enabled)},
+            'SRUnavailable'
+        )
+
+    def _update_minidrbdcluster_on_all_hosts(self, enabled):
+        master = util.get_master_ref(self.session)
+        self._update_minidrbdcluster(master, enabled)
+
+        for slave in util.get_all_slaves(self.session):
+            self._update_minidrbdcluster(slave, enabled)
 
     # --------------------------------------------------------------------------
     # Metadata.
@@ -1384,8 +1455,15 @@ class LinstorVDI(VDI.VDI):
         # 4. Create!
         failed = False
         try:
+            volume_name = None
+            if self.ty == 'ha_statefile':
+                volume_name = 'xcp-persistent-ha-statefile'
+            elif self.ty == 'redo_log':
+                volume_name = 'xcp-persistent-redo-log'
+
             self._linstor.create_volume(
-                self.uuid, volume_size, persistent=False
+                self.uuid, volume_size, persistent=False,
+                volume_name=volume_name
             )
             volume_info = self._linstor.get_volume_info(self.uuid)
 
@@ -1822,25 +1900,14 @@ class LinstorVDI(VDI.VDI):
         else:
             fn = 'attach' if attach else 'detach'
 
-            # We assume the first pool is always the one currently in use.
-            pools = self.session.xenapi.pool.get_all()
-            master = self.session.xenapi.pool.get_master(pools[0])
+            master = util.get_master_ref(self.session)
+
             args = {
                 'groupName': self.sr._group_name,
                 'srUuid': self.sr.uuid,
                 'vdiUuid': self.uuid
             }
-            ret = self.session.xenapi.host.call_plugin(
-                    master, self.sr.MANAGER_PLUGIN, fn, args
-            )
-            util.SMlog(
-                'call-plugin ({} with {}) returned: {}'.format(fn, args, ret)
-            )
-            if ret == 'False':
-                raise xs_errors.XenError(
-                    'VDIUnavailable',
-                    opterr='Plugin {} failed'.format(self.sr.MANAGER_PLUGIN)
-                )
+            self.sr._exec_manager_command(master, fn, args, 'VDIUnavailable')
 
         # Reload size attrs after inflate or deflate!
         self._load_this()
diff --git a/drivers/cleanup.py b/drivers/cleanup.py
index 895f36e8..9e3a5b07 100755
--- a/drivers/cleanup.py
+++ b/drivers/cleanup.py
@@ -52,8 +52,10 @@ from srmetadata import LVMMetadataHandler, VDI_TYPE_TAG
 try:
     from linstorjournaler import LinstorJournaler
     from linstorvhdutil import LinstorVhdUtil
-    from linstorvolumemanager \
-        import LinstorVolumeManager, LinstorVolumeManagerError
+    from linstorvolumemanager import get_controller_uri
+    from linstorvolumemanager import LinstorVolumeManager
+    from linstorvolumemanager import LinstorVolumeManagerError
+
     LINSTOR_AVAILABLE = True
 except ImportError:
     LINSTOR_AVAILABLE = False
@@ -2873,7 +2875,6 @@ class LinstorSR(SR):
             )
 
         SR.__init__(self, uuid, xapi, createLock, force)
-        self._master_uri = 'linstor://localhost'
         self.path = LinstorVolumeManager.DEV_ROOT_PATH
         self._reloadLinstor()
 
@@ -2918,12 +2919,13 @@ class LinstorSR(SR):
         dconf = session.xenapi.PBD.get_device_config(pbd)
         group_name = dconf['group-name']
 
+        controller_uri = get_controller_uri()
         self.journaler = LinstorJournaler(
-            self._master_uri, group_name, logger=util.SMlog
+            controller_uri, group_name, logger=util.SMlog
         )
 
         self._linstor = LinstorVolumeManager(
-            self._master_uri,
+            controller_uri,
             group_name,
             repair=True,
             logger=util.SMlog
diff --git a/drivers/linstor-manager b/drivers/linstor-manager
index e7e58fd8..f82b73f2 100755
--- a/drivers/linstor-manager
+++ b/drivers/linstor-manager
@@ -22,7 +22,7 @@ import XenAPIPlugin
 
 sys.path.append('/opt/xensource/sm/')
 from linstorjournaler import LinstorJournaler
-from linstorvolumemanager import LinstorVolumeManager
+from linstorvolumemanager import get_controller_uri, LinstorVolumeManager
 from lock import Lock
 import json
 import LinstorSR
@@ -34,10 +34,6 @@ FIREWALL_PORT_SCRIPT = '/etc/xapi.d/plugins/firewall-port'
 LINSTOR_PORTS = [3366, 3370, 3376, 3377, '7000:8000']
 
 
-def get_linstor_uri(session):
-    return 'linstor://{}'.format(util.get_master_rec(session)['address'])
-
-
 def update_port(port, open):
     fn = 'open' if open else 'close'
     args = (
@@ -55,23 +51,72 @@ def update_all_ports(open):
         update_port(port, open)
 
 
-def update_service(start):
+def enable_and_start_service(name, start):
     fn = 'enable' if start else 'disable'
-    args = ('systemctl', fn, '--now', 'linstor-satellite')
+    args = ('systemctl', fn, '--now', name)
     (ret, out, err) = util.doexec(args)
     if ret == 0:
         return
-    raise Exception('Failed to {} satellite: {} {}'.format(fn, out, err))
+    raise Exception('Failed to {} {}: {} {}'.format(fn, name, out, err))
+
+
+def restart_service(name):
+    args = ('systemctl', 'restart', name)
+    (ret, out, err) = util.doexec(args)
+    if ret == 0:
+        return
+    raise Exception('Failed to restart {}: {} {}'.format(name, out, err))
+
+
+def stop_service(name):
+    args = ('systemctl', 'stop', name)
+    (ret, out, err) = util.doexec(args)
+    if ret == 0:
+        return
+    raise Exception('Failed to stop {}: {} {}'.format(name, out, err))
+
+
+def update_linstor_satellite_service(start):
+    enable_and_start_service('linstor-satellite', start)
+
+
+def update_minidrbdcluster_service(start):
+    enable_and_start_service('minidrbdcluster', start)
+
+
+def prepare_sr(session, args):
+    try:
+        update_all_ports(open=True)
+        # We don't want to enable and start minidrbdcluster daemon during
+        # SR creation.
+        update_minidrbdcluster_service(start=False)
+        update_linstor_satellite_service(start=True)
+        return str(True)
+    except Exception as e:
+        util.SMlog('linstor-manager:prepare_sr error: {}'.format(e))
+    return str(False)
 
 
-def enable(session, args):
+def release_sr(session, args):
+    try:
+        update_linstor_satellite_service(start=False)
+        update_minidrbdcluster_service(start=False)
+        update_all_ports(open=False)
+        return str(True)
+    except Exception as e:
+        util.SMlog('linstor-manager:release_sr error: {}'.format(e))
+    return str(False)
+
+
+def update_minidrbdcluster(session, args):
     try:
         enabled = distutils.util.strtobool(args['enabled'])
-        update_all_ports(open=enabled)
-        update_service(start=enabled)
+        update_minidrbdcluster_service(start=enabled)
         return str(True)
     except Exception as e:
-        util.SMlog('linstor-manager:disable error: {}'.format(e))
+        util.SMlog(
+            'linstor-manager:update_minidrbdcluster error: {}'.format(e)
+        )
     return str(False)
 
 
@@ -81,12 +126,12 @@ def attach(session, args):
         vdi_uuid = args['vdiUuid']
         group_name = args['groupName']
 
-        linstor_uri = get_linstor_uri(session)
+        controller_uri = get_controller_uri()
         journaler = LinstorJournaler(
-            linstor_uri, group_name, logger=util.SMlog
+            controller_uri, group_name, logger=util.SMlog
         )
         linstor = LinstorVolumeManager(
-            linstor_uri,
+            controller_uri,
             group_name,
             logger=util.SMlog
         )
@@ -104,7 +149,7 @@ def detach(session, args):
         group_name = args['groupName']
 
         linstor = LinstorVolumeManager(
-            get_linstor_uri(session),
+            get_controller_uri(),
             group_name,
             logger=util.SMlog
         )
@@ -115,6 +160,29 @@ def detach(session, args):
     return str(False)
 
 
+def destroy(session, args):
+    try:
+        group_name = args['groupName']
+
+        # When destroy is called, there are no running minidrbdcluster daemons.
+        # So the controllers are stopped too, we must start an instance.
+        restart_service('var-lib-linstor.mount')
+        restart_service('linstor-controller')
+
+        linstor = LinstorVolumeManager(
+            'linstor://localhost',
+            group_name,
+            logger=util.SMlog
+        )
+        linstor.destroy()
+        return str(True)
+    except Exception as e:
+        stop_service('linstor-controller')
+        stop_service('var-lib-linstor.mount')
+        util.SMlog('linstor-manager:destroy error: {}'.format(e))
+    return str(False)
+
+
 def check(session, args):
     try:
         device_path = args['devicePath']
@@ -133,7 +201,7 @@ def get_vhd_info(session, args):
         include_parent = distutils.util.strtobool(args['includeParent'])
 
         linstor = LinstorVolumeManager(
-            get_linstor_uri(session),
+            get_controller_uri(),
             group_name,
             logger=util.SMlog
         )
@@ -168,7 +236,7 @@ def get_parent(session, args):
         group_name = args['groupName']
 
         linstor = LinstorVolumeManager(
-            get_linstor_uri(session),
+            get_controller_uri(),
             group_name,
             logger=util.SMlog
         )
@@ -244,7 +312,7 @@ def lock_vdi(session, args):
             lock.acquire()
 
         linstor = LinstorVolumeManager(
-            get_linstor_uri(session),
+            get_controller_uri(),
             group_name,
             logger=util.SMlog
         )
@@ -261,9 +329,12 @@ def lock_vdi(session, args):
 
 if __name__ == '__main__':
     XenAPIPlugin.dispatch({
-        'enable': enable,
+        'prepareSr': prepare_sr,
+        'releaseSr': release_sr,
+        'updateMinidrbdcluster': update_minidrbdcluster,
         'attach': attach,
         'detach': detach,
+        'destroy': destroy,
         'check': check,
         'getVHDInfo': get_vhd_info,
         'hasParent': has_parent,
diff --git a/drivers/linstorjournaler.py b/drivers/linstorjournaler.py
index 74953305..285012ca 100755
--- a/drivers/linstorjournaler.py
+++ b/drivers/linstorjournaler.py
@@ -16,7 +16,7 @@
 #
 
 
-from linstorvolumemanager import LinstorVolumeManager
+from linstorvolumemanager import get_controller_uri, LinstorVolumeManager
 import linstor
 import re
 import util
@@ -52,20 +52,10 @@ class LinstorJournaler:
         self._namespace = '{}journal/'.format(
             LinstorVolumeManager._build_sr_namespace()
         )
-
-        def connect():
-            self._journal = linstor.KV(
-                LinstorVolumeManager._build_group_name(group_name),
-                uri=uri,
-                namespace=self._namespace
-            )
-
-        util.retry(
-            connect,
-            maxretry=60,
-            exceptions=[linstor.errors.LinstorNetworkError]
-        )
         self._logger = logger
+        self._journal = self._create_journal_instance(
+            uri, group_name, self._namespace
+        )
 
     def create(self, type, identifier, value):
         # TODO: Maybe rename to 'add' in the future (in Citrix code too).
@@ -150,6 +140,28 @@ class LinstorJournaler:
     def _reset_namespace(self):
         self._journal.namespace = self._namespace
 
+    @classmethod
+    def _create_journal_instance(cls, uri, group_name, namespace):
+        def connect(uri):
+            if not uri:
+                uri = get_controller_uri()
+            return linstor.KV(
+                LinstorVolumeManager._build_group_name(group_name),
+                uri=uri,
+                namespace=namespace
+            )
+
+        try:
+            return connect(uri)
+        except linstor.errors.LinstorNetworkError:
+            pass
+
+        return util.retry(
+            lambda: connect(None),
+            maxretry=10,
+            exceptions=[linstor.errors.LinstorNetworkError]
+        )
+
     @staticmethod
     def _get_key(type, identifier):
         return '{}/{}'.format(type, identifier)
diff --git a/drivers/linstorvolumemanager.py b/drivers/linstorvolumemanager.py
index a6f67d8d..a383e327 100755
--- a/drivers/linstorvolumemanager.py
+++ b/drivers/linstorvolumemanager.py
@@ -16,15 +16,30 @@
 #
 
 
+import glob
 import json
 import linstor
 import os.path
 import re
+import shutil
 import socket
 import time
 import util
+import uuid
 
 
+# Contains the data of the "/var/lib/linstor" directory.
+DATABASE_VOLUME_NAME = 'xcp-persistent-database'
+DATABASE_SIZE = 1 << 30  # 1GB.
+DATABASE_PATH = '/var/lib/linstor'
+DATABASE_MKFS = 'mkfs.ext4'
+
+REG_DRBDADM_PRIMARY = re.compile("([^\\s]+)\\s+role:Primary")
+REG_DRBDSETUP_IP = re.compile('[^\\s]+\\s+(.*):.*$')
+
+
+# ==============================================================================
+
 def round_up(value, divisor):
     assert divisor
     divisor = int(divisor)
@@ -37,6 +52,79 @@ def round_down(value, divisor):
     return value - (value % int(divisor))
 
 
+# ==============================================================================
+
+def get_remote_host_ip(node_name):
+    (ret, stdout, stderr) = util.doexec([
+        'drbdsetup', 'show', DATABASE_VOLUME_NAME, '--json'
+    ])
+    if ret != 0:
+        return
+
+    try:
+        conf = json.loads(stdout)
+        if not conf:
+            return
+
+        for connection in conf[0]['connections']:
+            if connection['net']['_name'] == node_name:
+                value = connection['path']['_remote_host']
+                res = REG_DRBDSETUP_IP.match(value)
+                if res:
+                    return res.groups()[0]
+                break
+    except Exception:
+        pass
+
+
+def _get_controller_uri():
+    (ret, stdout, stderr) = util.doexec([
+        'drbdadm', 'status', DATABASE_VOLUME_NAME
+    ])
+    if ret != 0:
+        return
+
+    if stdout.startswith('{} role:Primary'.format(DATABASE_VOLUME_NAME)):
+        return 'linstor://localhost'
+
+    res = REG_DRBDADM_PRIMARY.search(stdout)
+    if res:
+        node_name = res.groups()[0]
+        ip = get_remote_host_ip(node_name)
+        if ip:
+            return 'linstor://' + ip
+
+
+def get_controller_uri():
+    retries = 0
+    while True:
+        uri = _get_controller_uri()
+        if uri:
+            return uri
+
+        retries += 1
+        if retries >= 10:
+            break
+        time.sleep(1)
+
+
+def get_controller_node_name():
+    (ret, stdout, stderr) = util.doexec([
+        'drbdadm', 'status', DATABASE_VOLUME_NAME
+    ])
+    if ret != 0:
+        return
+
+    if stdout.startswith('{} role:Primary'.format(DATABASE_VOLUME_NAME)):
+        return 'localhost'
+
+    res = REG_DRBDADM_PRIMARY.search(stdout)
+    if res:
+        return res.groups()[0]
+
+
+# ==============================================================================
+
 class LinstorVolumeManagerError(Exception):
     ERR_GENERIC = 0,
     ERR_VOLUME_EXISTS = 1,
@@ -50,6 +138,7 @@ class LinstorVolumeManagerError(Exception):
     def code(self):
         return self._code
 
+
 # ==============================================================================
 
 # Note:
@@ -152,7 +241,7 @@ class LinstorVolumeManager(object):
         self, uri, group_name, repair=False, logger=default_logger.__func__
     ):
         """
-        Create a new LinstorApi object.
+        Create a new LinstorVolumeManager object.
         :param str uri: URI to communicate with the LINSTOR controller.
         :param str group_name: The SR goup name to use.
         :param bool repair: If true we try to remove bad volumes due to a crash
@@ -160,7 +249,6 @@ class LinstorVolumeManager(object):
         :param function logger: Function to log messages.
         """
 
-        self._uri = uri
         self._linstor = self._create_linstor_instance(uri)
         self._base_group_name = group_name
 
@@ -266,7 +354,7 @@ class LinstorVolumeManager(object):
                     )
                 if size is None or current_size < size:
                     size = current_size
-        return size * 1024
+        return (size or 0) * 1024
 
     @property
     def total_volume_size(self):
@@ -379,19 +467,24 @@ class LinstorVolumeManager(object):
         """
         return volume_uuid in self._volumes
 
-    def create_volume(self, volume_uuid, size, persistent=True):
+    def create_volume(
+        self, volume_uuid, size, persistent=True, volume_name=None
+    ):
         """
         Create a new volume on the SR.
         :param str volume_uuid: The volume uuid to use.
         :param int size: volume size in B.
         :param bool persistent: If false the volume will be unavailable
         on the next constructor call LinstorSR(...).
+        :param str volume_name: If set, this name is used in the LINSTOR
+        database instead of a generated name.
         :return: The current device path of the volume.
         :rtype: str
         """
 
         self._logger('Creating LINSTOR volume {}...'.format(volume_uuid))
-        volume_name = self.build_volume_name(util.gen_uuid())
+        if not volume_name:
+            volume_name = self.build_volume_name(util.gen_uuid())
         volume_properties = self._create_volume_with_properties(
             volume_uuid, volume_name, size, place_resources=True
         )
@@ -1073,23 +1166,56 @@ class LinstorVolumeManager(object):
             if not volume_name or volume_name not in resource_names:
                 self.destroy_volume(volume_uuid)
 
-    def destroy(self, force=False):
+    def destroy(self):
         """
         Destroy this SR. Object should not be used after that.
         :param bool force: Try to destroy volumes before if true.
         """
 
-        if (force):
-            for volume_uuid in self._volumes:
-                self.destroy_volume(volume_uuid)
+        if self._volumes:
+            raise LinstorVolumeManagerError(
+                'Cannot destroy LINSTOR volume manager: '
+                'It exists remaining volumes'
+            )
 
-        # TODO: Throw exceptions in the helpers below if necessary.
-        # TODO: What's the required action if it exists remaining volumes?
+        uri = 'linstor://localhost'
+        try:
+            self._start_controller(start=False)
 
-        self._destroy_resource_group(self._linstor, self._group_name)
-        for pool in self._get_storage_pools(force=True):
-            self._destroy_storage_pool(
-                self._linstor, pool.name, pool.node_name
+            # 1. Umount LINSTOR database.
+            self._mount_database_volume(
+                self.build_device_path(DATABASE_VOLUME_NAME),
+                mount=False,
+                force=True
+            )
+
+            # 2. Refresh instance.
+            self._start_controller(start=True)
+            self._linstor = self._create_linstor_instance(
+                uri, keep_uri_unmodified=True
+            )
+
+            # 3. Destroy database volume.
+            self._destroy_resource(DATABASE_VOLUME_NAME)
+
+            # 4. Destroy group and storage pools.
+            self._destroy_resource_group(self._linstor, self._group_name)
+            for pool in self._get_storage_pools(force=True):
+                self._destroy_storage_pool(
+                    self._linstor, pool.name, pool.node_name
+                )
+        except Exception as e:
+            self._start_controller(start=True)
+            raise e
+
+        try:
+            self._start_controller(start=False)
+            for file in glob.glob(DATABASE_PATH + '/'):
+                os.remove(file)
+        except Exception as e:
+            util.SMlog(
+                'Ignoring failure after LINSTOR SR destruction: {}'
+                .format(e)
             )
 
     def find_up_to_date_diskfull_nodes(self, volume_uuid):
@@ -1130,29 +1256,75 @@ class LinstorVolumeManager(object):
 
     @classmethod
     def create_sr(
-        cls, uri, group_name, node_names, redundancy,
+        cls, group_name, node_names, ips, redundancy,
         thin_provisioning=False,
         logger=default_logger.__func__
     ):
         """
         Create a new SR on the given nodes.
-        :param str uri: URI to communicate with the LINSTOR controller.
         :param str group_name: The SR group_name to use.
         :param list[str] node_names: String list of nodes.
         :param int redundancy: How many copy of volumes should we store?
+        :param set(str) ips: Node ips
         :param function logger: Function to log messages.
         :return: A new LinstorSr instance.
         :rtype: LinstorSr
         """
 
+        try:
+            cls._start_controller(start=True)
+            sr = cls._create_sr(
+                group_name,
+                node_names,
+                ips,
+                redundancy,
+                thin_provisioning,
+                logger
+            )
+        finally:
+            # Controller must be stopped and volume unmounted because
+            # it is the role of the minidrbdcluster daemon to do the right
+            # actions.
+            cls._start_controller(start=False)
+            cls._mount_volume(
+                cls.build_device_path(DATABASE_VOLUME_NAME),
+                DATABASE_PATH,
+                mount=False
+            )
+        return sr
+
+    @classmethod
+    def _create_sr(
+        cls, group_name, node_names, ips, redundancy,
+        thin_provisioning=False,
+        logger=default_logger.__func__
+    ):
         # 1. Check if SR already exists.
-        lin = cls._create_linstor_instance(uri)
+        uri = 'linstor://localhost'
+
+        lin = cls._create_linstor_instance(uri, keep_uri_unmodified=True)
+
+        for node_name in node_names:
+            ip = ips[node_name]
+            result = lin.node_create(
+                node_name,
+                linstor.consts.VAL_NODE_TYPE_CMBD,
+                ip
+            )
+            errors = cls._filter_errors(result)
+            if cls._check_errors(errors, [linstor.consts.FAIL_EXISTS_NODE]):
+                continue
+
+            if errors:
+                raise LinstorVolumeManagerError(
+                    'Failed to create node `{}` with ip `{}`: {}'.format(
+                        node_name, ip, cls._get_error_str(errors)
+                    )
+                )
+
         driver_pool_name = group_name
         group_name = cls._build_group_name(group_name)
         pools = lin.storage_pool_list_raise(filter_by_stor_pools=[group_name])
-
-        # TODO: Maybe if the SR already exists and if the nodes are the same,
-        # we can try to use it directly.
         pools = pools.storage_pools
         if pools:
             existing_node_names = map(lambda pool: pool.node_name, pools)
@@ -1227,25 +1399,64 @@ class LinstorVolumeManager(object):
                     )
                 )
 
-        # 3. Remove storage pools/resource/volume group in the case of errors.
+            # 3. Create the LINSTOR database volume and mount it.
+            try:
+                logger('Creating database volume...')
+                volume_path = cls._create_database_volume(lin, group_name)
+            except LinstorVolumeManagerError as e:
+                if e.code != LinstorVolumeManagerError.ERR_VOLUME_EXISTS:
+                    logger('Destroying database volume after creation fail...')
+                    cls._force_destroy_database_volume(lin, group_name)
+                raise
+
+            try:
+                logger('Mounting database volume...')
+
+                # First we must disable the controller to move safely the
+                # LINSTOR config.
+                cls._start_controller(start=False)
+
+                cls._mount_database_volume(volume_path)
+            except Exception as e:
+                # Ensure we are connected because controller has been
+                # restarted during mount call.
+                logger('Destroying database volume after mount fail...')
+
+                try:
+                    cls._start_controller(start=True)
+                except Exception:
+                    pass
+
+                lin = cls._create_linstor_instance(
+                    uri, keep_uri_unmodified=True
+                )
+                cls._force_destroy_database_volume(lin, group_name)
+                raise e
+
+            cls._start_controller(start=True)
+            lin = cls._create_linstor_instance(uri, keep_uri_unmodified=True)
+
+        # 4. Remove storage pools/resource/volume group in the case of errors.
         except Exception as e:
+            logger('Destroying resource group and storage pools after fail...')
             try:
                 cls._destroy_resource_group(lin, group_name)
-            except Exception:
+            except Exception as e2:
+                logger('Failed to destroy resource group: {}'.format(e2))
                 pass
             j = 0
             i = min(i, len(node_names) - 1)
             while j <= i:
                 try:
                     cls._destroy_storage_pool(lin, group_name, node_names[j])
-                except Exception:
+                except Exception as e2:
+                    logger('Failed to destroy resource group: {}'.format(e2))
                     pass
                 j += 1
             raise e
 
-        # 4. Return new instance.
+        # 5. Return new instance.
         instance = cls.__new__(cls)
-        instance._uri = uri
         instance._linstor = lin
         instance._logger = logger
         instance._redundancy = redundancy
@@ -1462,26 +1673,6 @@ class LinstorVolumeManager(object):
 
         return self._storage_pools
 
-    def _check_volume_creation_errors(self, result, volume_uuid):
-        errors = self._filter_errors(result)
-        if self._check_errors(errors, [
-            linstor.consts.FAIL_EXISTS_RSC, linstor.consts.FAIL_EXISTS_RSC_DFN
-        ]):
-            raise LinstorVolumeManagerError(
-                'Failed to create volume `{}` from SR `{}`, it already exists'
-                .format(volume_uuid, self._group_name),
-                LinstorVolumeManagerError.ERR_VOLUME_EXISTS
-            )
-
-        if errors:
-            raise LinstorVolumeManagerError(
-                'Failed to create volume `{}` from SR `{}`: {}'.format(
-                    volume_uuid,
-                    self._group_name,
-                    self._get_error_str(errors)
-                )
-            )
-
     def _create_volume(self, volume_uuid, volume_name, size, place_resources):
         size = self.round_up_volume_size(size)
 
@@ -1491,7 +1682,7 @@ class LinstorVolumeManager(object):
             rsc_dfn_name=volume_name,
             vlm_sizes=['{}B'.format(size)],
             definitions_only=not place_resources
-        ), volume_uuid)
+        ), volume_uuid, self._group_name)
 
     def _create_volume_with_properties(
         self, volume_uuid, volume_name, size, place_resources
@@ -1535,12 +1726,8 @@ class LinstorVolumeManager(object):
             # before the `self._create_volume` case.
             # It can only happen if the same volume uuid is used in the same
             # call in another host.
-            if e.code == LinstorVolumeManagerError.ERR_VOLUME_EXISTS:
-                raise
-            self._force_destroy_volume(volume_uuid)
-            raise
-        except Exception:
-            self._force_destroy_volume(volume_uuid)
+            if e.code != LinstorVolumeManagerError.ERR_VOLUME_EXISTS:
+                self._force_destroy_volume(volume_uuid)
             raise
 
     def _find_device_path(self, volume_uuid, volume_name):
@@ -1576,7 +1763,10 @@ class LinstorVolumeManager(object):
 
         if not resources:
             if activate:
-                self._activate_device_path(node_name, volume_name)
+                self._mark_resource_cache_as_dirty()
+                self._activate_device_path(
+                    self._linstor, node_name, volume_name
+                )
                 return self._request_device_path(volume_uuid, volume_name)
             raise LinstorVolumeManagerError(
                 'Empty dev path for `{}`, but definition "seems" to exist'
@@ -1585,25 +1775,6 @@ class LinstorVolumeManager(object):
         # Contains a path of the /dev/drbd<id> form.
         return resources[0].volumes[0].device_path
 
-    def _activate_device_path(self, node_name, volume_name):
-        self._mark_resource_cache_as_dirty()
-        result = self._linstor.resource_create([
-            linstor.ResourceData(node_name, volume_name, diskless=True)
-        ])
-        if linstor.Linstor.all_api_responses_no_error(result):
-            return
-        errors = linstor.Linstor.filter_api_call_response_errors(result)
-        if len(errors) == 1 and errors[0].is_error(
-            linstor.consts.FAIL_EXISTS_RSC
-        ):
-            return
-
-        raise LinstorVolumeManagerError(
-            'Unable to activate device path of `{}` on node `{}`: {}'
-            .format(volume_name, node_name, ', '.join(
-                [str(x) for x in result]))
-        )
-
     def _destroy_resource(self, resource_name):
         self._mark_resource_cache_as_dirty()
         result = self._linstor.resource_dfn_delete(resource_name)
@@ -1757,7 +1928,7 @@ class LinstorVolumeManager(object):
     def _create_linstor_kv(self, namespace):
         return linstor.KV(
             self._get_store_name(),
-            uri=self._uri,
+            uri=self._linstor.controller_host(),
             namespace=namespace
         )
 
@@ -1787,46 +1958,347 @@ class LinstorVolumeManager(object):
         ])
 
     @classmethod
-    def _create_linstor_instance(cls, uri):
-        def connect():
+    def _create_linstor_instance(cls, uri, keep_uri_unmodified=False):
+        retry = False
+
+        def connect(uri):
+            if not uri:
+                uri = get_controller_uri()
+                if not uri:
+                    raise LinstorVolumeManagerError(
+                        'Unable to find controller uri...'
+                    )
             instance = linstor.Linstor(uri, keep_alive=True)
             instance.connect()
             return instance
 
+        try:
+            return connect(uri)
+        except (linstor.errors.LinstorNetworkError, LinstorVolumeManagerError):
+            pass
+
+        if not keep_uri_unmodified:
+            uri = None
+
         return util.retry(
-            connect,
-            maxretry=60,
-            exceptions=[linstor.errors.LinstorNetworkError]
+            lambda: connect(uri),
+            maxretry=10,
+            exceptions=[
+                linstor.errors.LinstorNetworkError,
+                LinstorVolumeManagerError
+            ]
         )
 
     @classmethod
-    def _destroy_storage_pool(cls, lin, group_name, node_name):
-        result = lin.storage_pool_delete(node_name, group_name)
+    def _activate_device_path(cls, lin, node_name, volume_name):
+        result = lin.resource_create([
+            linstor.ResourceData(node_name, volume_name, diskless=True)
+        ])
+        if linstor.Linstor.all_api_responses_no_error(result):
+            return
+        errors = linstor.Linstor.filter_api_call_response_errors(result)
+        if len(errors) == 1 and errors[0].is_error(
+            linstor.consts.FAIL_EXISTS_RSC
+        ):
+            return
+
+        raise LinstorVolumeManagerError(
+            'Unable to activate device path of `{}` on node `{}`: {}'
+            .format(volume_name, node_name, ', '.join(
+                [str(x) for x in result]))
+            )
+
+    @classmethod
+    def _request_database_path(cls, lin, activate=False):
+        node_name = socket.gethostname()
+
+        try:
+            resources = filter(
+                lambda resource: resource.node_name == node_name and
+                resource.name == DATABASE_VOLUME_NAME,
+                lin.resource_list_raise().resources
+            )
+        except Exception as e:
+            raise LinstorVolumeManagerError(
+                'Unable to get resources during database creation: {}'
+                .format(e)
+            )
+
+        if not resources:
+            if activate:
+                cls._activate_device_path(
+                    lin, node_name, DATABASE_VOLUME_NAME
+                )
+                return cls._request_database_path(
+                    DATABASE_VOLUME_NAME, DATABASE_VOLUME_NAME
+                )
+            raise LinstorVolumeManagerError(
+                'Empty dev path for `{}`, but definition "seems" to exist'
+                .format(DATABASE_PATH)
+            )
+        # Contains a path of the /dev/drbd<id> form.
+        return resources[0].volumes[0].device_path
+
+    @classmethod
+    def _create_database_volume(cls, lin, group_name):
+        try:
+            dfns = lin.resource_dfn_list_raise().resource_definitions
+        except Exception as e:
+            raise LinstorVolumeManagerError(
+                'Unable to get definitions during database creation: {}'
+                .format(e)
+            )
+
+        if dfns:
+            raise LinstorVolumeManagerError(
+                'Could not create volume `{}` from SR `{}`, '.format(
+                    DATABASE_VOLUME_NAME, group_name
+                ) + 'LINSTOR volume list must be empty.'
+            )
+
+        size = cls.round_up_volume_size(DATABASE_SIZE)
+        cls._check_volume_creation_errors(lin.resource_group_spawn(
+            rsc_grp_name=group_name,
+            rsc_dfn_name=DATABASE_VOLUME_NAME,
+            vlm_sizes=['{}B'.format(size)],
+            definitions_only=False
+        ), DATABASE_VOLUME_NAME, group_name)
+
+        # We must modify the quorum. Otherwise we can't use correctly the
+        # minidrbdcluster daemon.
+        result = lin.resource_dfn_modify(DATABASE_VOLUME_NAME, {
+            'DrbdOptions/auto-quorum': 'disabled',
+            'DrbdOptions/Resource/quorum': 'majority'
+        })
         error_str = cls._get_error_str(result)
         if error_str:
             raise LinstorVolumeManagerError(
-                'Failed to destroy SP `{}` on node `{}`: {}'.format(
-                    group_name,
-                    node_name,
-                    error_str
+                'Could not activate quorum on database volume: {}'
+                .format(error_str)
+            )
+
+        current_device_path = cls._request_database_path(lin, activate=True)
+
+        # We use realpath here to get the /dev/drbd<id> path instead of
+        # /dev/drbd/by-res/<resource_name>.
+        expected_device_path = cls.build_device_path(DATABASE_VOLUME_NAME)
+        util.wait_for_path(expected_device_path, 5)
+
+        device_realpath = os.path.realpath(expected_device_path)
+        if current_device_path != device_realpath:
+            raise LinstorVolumeManagerError(
+                'Invalid path, current={}, expected={} (realpath={})'
+                .format(
+                    current_device_path,
+                    expected_device_path,
+                    device_realpath
                 )
             )
 
+        try:
+            util.pread2([DATABASE_MKFS, expected_device_path])
+        except Exception as e:
+            raise LinstorVolumeManagerError(
+               'Failed to execute {} on database volume: {}'
+               .format(DATABASE_MKFS, e)
+            )
+
+        return expected_device_path
+
     @classmethod
-    def _destroy_resource_group(cls, lin, group_name):
-        result = lin.resource_group_delete(group_name)
-        error_str = cls._get_error_str(result)
+    def _destroy_database_volume(cls, lin, group_name):
+        error_str = cls._get_error_str(
+            lin.resource_dfn_delete(DATABASE_VOLUME_NAME)
+        )
         if error_str:
             raise LinstorVolumeManagerError(
-                'Failed to destroy RG `{}`: {}'.format(group_name, error_str)
+                'Could not destroy resource `{}` from SR `{}`: {}'
+                .format(DATABASE_VOLUME_NAME, group_name, error_str)
             )
 
+    @classmethod
+    def _mount_database_volume(cls, volume_path, mount=True, force=False):
+        backup_path = DATABASE_PATH + '-' + str(uuid.uuid4())
+
+        try:
+            # 1. Create a backup config folder.
+            database_not_empty = bool(os.listdir(DATABASE_PATH))
+            if database_not_empty:
+                try:
+                    os.mkdir(backup_path)
+                except Exception as e:
+                    raise LinstorVolumeManagerError(
+                        'Failed to create backup path {} of LINSTOR config: {}'
+                        .format(backup_path, e)
+                    )
+
+            # 2. Move the config in the mounted volume.
+            if database_not_empty:
+                cls._move_files(DATABASE_PATH, backup_path)
+
+            cls._mount_volume(volume_path, DATABASE_PATH, mount)
+
+            if database_not_empty:
+                cls._move_files(backup_path, DATABASE_PATH, force)
+
+                # 3. Remove useless backup directory.
+                try:
+                    os.rmdir(backup_path)
+                except Exception:
+                    raise LinstorVolumeManagerError(
+                        'Failed to remove backup path {} of LINSTOR config {}'
+                        .format(backup_path, e)
+                    )
+        except Exception as e:
+            def force_exec(fn):
+                try:
+                    fn()
+                except Exception:
+                    pass
+
+            if mount == cls._is_mounted(DATABASE_PATH):
+                force_exec(lambda: cls._move_files(
+                    DATABASE_PATH, backup_path
+                ))
+                force_exec(lambda: cls._mount_volume(
+                    volume_path, DATABASE_PATH, not mount
+                ))
+
+            if mount != cls._is_mounted(DATABASE_PATH):
+                force_exec(lambda: cls._move_files(
+                    backup_path, DATABASE_PATH
+                ))
+
+            force_exec(lambda: os.rmdir(backup_path))
+            raise e
+
+    @classmethod
+    def _force_destroy_database_volume(cls, lin, group_name):
+        try:
+            cls._destroy_database_volume(lin, group_name)
+        except Exception:
+            pass
+
+    @classmethod
+    def _destroy_storage_pool(cls, lin, group_name, node_name):
+        def destroy():
+            result = lin.storage_pool_delete(node_name, group_name)
+            errors = cls._filter_errors(result)
+            if cls._check_errors(errors, [
+                linstor.consts.FAIL_NOT_FOUND_STOR_POOL,
+                linstor.consts.FAIL_NOT_FOUND_STOR_POOL_DFN
+            ]):
+                return
+
+            if errors:
+                raise LinstorVolumeManagerError(
+                    'Failed to destroy SP `{}` on node `{}`: {}'.format(
+                        group_name,
+                        node_name,
+                        cls._get_error_str(errors)
+                    )
+                )
+
+        # We must retry to avoid errors like:
+        # "can not be deleted as volumes / snapshot-volumes are still using it"
+        # after LINSTOR database volume destruction.
+        return util.retry(destroy, maxretry=10)
+
+    @classmethod
+    def _destroy_resource_group(cls, lin, group_name):
+        def destroy():
+            result = lin.resource_group_delete(group_name)
+            errors = cls._filter_errors(result)
+            if cls._check_errors(errors, [
+                linstor.consts.FAIL_NOT_FOUND_RSC_GRP
+            ]):
+                return
+
+            if errors:
+                raise LinstorVolumeManagerError(
+                    'Failed to destroy RG `{}`: {}'
+                    .format(group_name, cls._get_error_str(errors))
+                )
+
+        return util.retry(destroy, maxretry=10)
+
     @classmethod
     def _build_group_name(cls, base_name):
         # If thin provisioning is used we have a path like this:
         # `VG/LV`. "/" is not accepted by LINSTOR.
         return '{}{}'.format(cls.PREFIX_SR, base_name.replace('/', '_'))
 
+    @classmethod
+    def _check_volume_creation_errors(cls, result, volume_uuid, group_name):
+        errors = cls._filter_errors(result)
+        if cls._check_errors(errors, [
+            linstor.consts.FAIL_EXISTS_RSC, linstor.consts.FAIL_EXISTS_RSC_DFN
+        ]):
+            raise LinstorVolumeManagerError(
+                'Failed to create volume `{}` from SR `{}`, it already exists'
+                .format(volume_uuid, group_name),
+                LinstorVolumeManagerError.ERR_VOLUME_EXISTS
+            )
+
+        if errors:
+            raise LinstorVolumeManagerError(
+                'Failed to create volume `{}` from SR `{}`: {}'.format(
+                    volume_uuid,
+                    group_name,
+                    cls._get_error_str(errors)
+                )
+            )
+
+    @classmethod
+    def _move_files(cls, src_dir, dest_dir, force=False):
+        def listdir(dir):
+            ignored = ['lost+found']
+            return filter(lambda file: file not in ignored, os.listdir(dir))
+
+        try:
+            if not force:
+                files = listdir(dest_dir)
+                if files:
+                    raise LinstorVolumeManagerError(
+                        'Cannot move files from {} to {} because destination '
+                        'contains: {}'.format(src_dir, dest_dir, files)
+                    )
+        except LinstorVolumeManagerError:
+            raise
+        except Exception as e:
+            raise LinstorVolumeManagerError(
+                'Cannot list dir {}: {}'.format(dest_dir, e)
+            )
+
+        try:
+            for file in listdir(src_dir):
+                try:
+                    dest_file = os.path.join(dest_dir, file)
+                    if not force and os.path.exists(dest_file):
+                        raise LinstorVolumeManagerError(
+                            'Cannot move {} because it already exists in the '
+                            'destination'.format(file)
+                        )
+                    shutil.move(os.path.join(src_dir, file), dest_file)
+                except LinstorVolumeManagerError:
+                    raise
+                except Exception as e:
+                    raise LinstorVolumeManagerError(
+                        'Cannot move {}: {}'.format(file, e)
+                    )
+        except Exception as e:
+            if not force:
+                try:
+                    cls._move_files(dest_dir, src_dir, force=True)
+                except Exception:
+                    pass
+
+            raise LinstorVolumeManagerError(
+                'Failed to move files from {} to {}: {}'.format(
+                    src_dir, dest_dir, e
+                )
+            )
+
     @staticmethod
     def _get_filtered_properties(properties):
         return dict(properties.items())
@@ -1845,3 +2317,44 @@ class LinstorVolumeManager(object):
                 if err.is_error(code):
                     return True
         return False
+
+    @classmethod
+    def _start_controller(cls, start=True):
+        return cls._start_service('linstor-controller', start)
+
+    @staticmethod
+    def _start_service(name, start=True):
+        action = 'start' if start else 'stop'
+        (ret, out, err) = util.doexec([
+            'systemctl', action, name
+        ])
+        if ret != 0:
+            raise LinstorVolumeManagerError(
+                'Failed to {} {}: {} {}'
+                .format(action, name, out, err)
+            )
+
+    @staticmethod
+    def _is_mounted(mountpoint):
+        (ret, out, err) = util.doexec(['mountpoint', '-q', mountpoint])
+        return ret == 0
+
+    @classmethod
+    def _mount_volume(cls, volume_path, mountpoint, mount=True):
+        if mount:
+            try:
+                util.pread(['mount', volume_path, mountpoint])
+            except Exception as e:
+                raise LinstorVolumeManagerError(
+                    'Failed to mount volume {} on {}: {}'
+                    .format(volume_path, mountpoint, e)
+                )
+        else:
+            try:
+                if cls._is_mounted(mountpoint):
+                    util.pread(['umount', mountpoint])
+            except Exception as e:
+                raise LinstorVolumeManagerError(
+                    'Failed to umount volume {} on {}: {}'
+                    .format(volume_path, mountpoint, e)
+                )
diff --git a/drivers/tapdisk-pause b/drivers/tapdisk-pause
index ed6abede..e0bca7be 100755
--- a/drivers/tapdisk-pause
+++ b/drivers/tapdisk-pause
@@ -30,7 +30,7 @@ import vhdutil
 import lvmcache
 
 try:
-    from linstorvolumemanager import LinstorVolumeManager
+    from linstorvolumemanager import get_controller_uri, LinstorVolumeManager
     LINSTOR_AVAILABLE = True
 except ImportError:
     LINSTOR_AVAILABLE = False
@@ -152,10 +152,6 @@ class Tapdisk:
             # "B" path. Note: "A", "B" and "OLD_A" are UUIDs.
             session = self.session
 
-            linstor_uri = 'linstor://{}'.format(
-                util.get_master_rec(session)['address']
-            )
-
             host_ref = util.get_this_host_ref(session)
             sr_ref = session.xenapi.SR.get_by_uuid(self.sr_uuid)
 
@@ -167,7 +163,7 @@ class Tapdisk:
             group_name = dconf['group-name']
 
             device_path = LinstorVolumeManager(
-                linstor_uri,
+                get_controller_uri(),
                 group_name,
                 logger=util.SMlog
             ).get_device_path(self.vdi_uuid)
diff --git a/drivers/util.py b/drivers/util.py
index 54fda469..7151f368 100755
--- a/drivers/util.py
+++ b/drivers/util.py
@@ -659,31 +659,10 @@ def get_master_ref(session):
     return session.xenapi.pool.get_master(pools[0])
 
 
-def get_master_rec(session):
-    return session.xenapi.host.get_record(get_master_ref(session))
-
-
 def is_master(session):
     return get_this_host_ref(session) == get_master_ref(session)
 
 
-def get_master_address():
-    address = None
-    try:
-        fd = open('/etc/xensource/pool.conf', 'r')
-        try:
-            items = fd.readline().split(':')
-            if items[0].strip() == 'master':
-                address = 'localhost'
-            else:
-                address = items[1].strip()
-        finally:
-            fd.close()
-    except Exception:
-        pass
-    return address
-
-
 # XXX: this function doesn't do what it claims to do
 def get_localhost_uuid(session):
     filename = '/etc/xensource-inventory'
diff --git a/etc/minidrbdcluster.ini b/etc/minidrbdcluster.ini
new file mode 100644
index 00000000..0126e862
--- /dev/null
+++ b/etc/minidrbdcluster.ini
@@ -0,0 +1,14 @@
+# minidrbdcluster keeps a service running on one of the nodes.
+# Quorum must be enabled in the DRBD resource!
+#
+# The section names are the names of DRBD resources. Within a
+# section name the systemd-units to activate on one of the nodes.
+
+[xcp-persistent-database]
+systemd-units=var-lib-linstor.mount,linstor-controller.service
+
+[xcp-persistent-ha-statefile]
+systemd-units=
+
+[xcp-persistent-redo-log]
+systemd-units=
diff --git a/etc/systemd/system/linstor-satellite.service.d/override.conf b/etc/systemd/system/linstor-satellite.service.d/override.conf
new file mode 100644
index 00000000..b1686b4f
--- /dev/null
+++ b/etc/systemd/system/linstor-satellite.service.d/override.conf
@@ -0,0 +1,5 @@
+[Service]
+Environment=LS_KEEP_RES=^xcp-persistent*
+
+[Unit]
+After=drbd.service
diff --git a/etc/systemd/system/var-lib-linstor.mount b/etc/systemd/system/var-lib-linstor.mount
new file mode 100644
index 00000000..a05a7f74
--- /dev/null
+++ b/etc/systemd/system/var-lib-linstor.mount
@@ -0,0 +1,6 @@
+[Unit]
+Description=Filesystem for the LINSTOR controller
+
+[Mount]
+What=/dev/drbd/by-res/xcp-persistent-database/0
+Where=/var/lib/linstor
diff --git a/linstor/linstor-monitord.c b/linstor/linstor-monitord.c
index a1592fda..47740598 100644
--- a/linstor/linstor-monitord.c
+++ b/linstor/linstor-monitord.c
@@ -287,18 +287,6 @@ static inline int addInotifyWatch (int inotifyFd, const char *filepath, uint32_t
 
 // -----------------------------------------------------------------------------
 
-static inline int updateLinstorController (int isMaster) {
-  syslog(LOG_INFO, "%s linstor-controller...", isMaster ? "Enabling" : "Disabling");
-  char *argv[] = {
-    "systemctl",
-    isMaster ? "enable" : "disable",
-    "--now",
-    "linstor-controller",
-    NULL
-  };
-  return execCommand(argv, NULL);
-}
-
 static inline int updateLinstorNode (State *state) {
   char buffer[256];
   if (gethostname(buffer, sizeof buffer) == -1) {
@@ -416,7 +404,6 @@ static inline int processPoolConfEvents (State *state, int wd, char **buffer, si
       inotify_rm_watch(state->inotifyFd, wd); // Do not forget to remove watch to avoid leaks.
       return -EIO;
     }
-    ret = updateLinstorController(state->isMaster);
   } else {
     if (mask & (IN_CREATE | IN_MOVED_TO)) {
       syslog(LOG_ERR, "Watched `" POOL_CONF_ABS_FILE "` file has been recreated!");
@@ -495,8 +482,6 @@ static inline int waitForPoolConfCreation (State *state, int *wdFile) {
       // Update LINSTOR services...
       int ret;
       state->isMaster = isMasterHost(&ret);
-      if (!ret)
-        ret = updateLinstorController(state->isMaster);
 
       // Ok we can't read the pool configuration file.
       // Maybe the file doesn't exist. Waiting its creation...
diff --git a/scripts/minidrbdcluster b/scripts/minidrbdcluster
new file mode 100755
index 00000000..a04b6c1c
--- /dev/null
+++ b/scripts/minidrbdcluster
@@ -0,0 +1,171 @@
+#! /usr/bin/env python2
+
+import configparser
+import os
+import re
+import signal
+import subprocess
+
+DRBDADM_OPEN_FAILED_RE = re.compile(
+    'open\\((.*)\\) failed: No such file or directory'
+)
+MAY_PROMOT_RE = re.compile(
+    '(?:exists|change) resource name:((?:\\w|-)+) '
+    '(?:\\w+\\:\\w+ )*may_promote:(yes|no) promotion_score:(\\d+)'
+)
+PEER_ROLE_RE = re.compile(
+    '(?:exists|change) connection name:((?:\\w|-)+) peer-node-id:(?:\\d+) '
+    'conn-name:(\\w+) (?:\\w+\\:\\w+ )*role:(Primary|Secondary|Unknown)'
+)
+HAVE_QUORUM_RE = re.compile(
+    '(?:exists|change) device name:((?:\\w|-)+) '
+    '(?:\\w+\\:\\w+ )*quorum:(yes|no)'
+)
+
+
+class SigHupException(Exception):
+    pass
+
+
+def sig_handler(sig, frame):
+    raise SigHupException(
+        'Received signal ' + str(sig) +
+        ' on line ' + str(frame.f_lineno) +
+        ' in ' + frame.f_code.co_filename
+    )
+
+
+def call_systemd(operation, service):
+    verbose = operation in ('start', 'stop')
+    if verbose:
+        print('Trying to %s %s' % (operation, service))
+    r = os.system('systemctl %s %s' % (operation, service))
+    if verbose:
+        print('%s for %s %s' % (
+            'success' if r == 0 else 'failure', operation, service
+        ))
+    return r == 0
+
+
+def ensure_systemd_started(service):
+    args = ['systemctl', 'is-active', '--quiet', service]
+
+    proc = subprocess.Popen(args)
+    proc.wait()
+    if not proc.returncode:
+        return True  # Already active.
+
+    return call_systemd('start', service)
+
+
+def show_status(services, status):
+    print('status:')
+    for systemd_unit in services:
+        call_systemd('status', systemd_unit)
+    for res_name in status:
+        print('%s is %s' % (res_name, status[res_name]))
+
+
+def clean_up(services):
+    print('exiting:')
+    for systemd_unit in reversed(services):
+        call_systemd('stop', systemd_unit)
+
+
+def get_systemd_units(systemd_units_str):
+    systemd_units = []
+    for systemd_unit in systemd_units_str.split(','):
+        systemd_unit = systemd_unit.strip()
+        if systemd_unit:
+            systemd_units.append(systemd_unit)
+    return systemd_units
+
+
+def process(events2, resources, services, status):
+    line = events2.stdout.readline()
+    m = MAY_PROMOT_RE.match(line)
+    if m:
+        res_name, may_promote, promotion_score = m.groups()
+        if res_name in resources and may_promote == 'yes':
+            systemd_units_str = resources[res_name]['systemd-units']
+            for systemd_unit in get_systemd_units(systemd_units_str):
+                if not ensure_systemd_started(systemd_unit):
+                    break
+                if systemd_unit not in services:
+                    services.append(systemd_unit)
+    m = PEER_ROLE_RE.match(line)
+    if m:
+        res_name, conn_name, role = m.groups()
+        if res_name in status:
+            status[res_name][conn_name] = role
+    m = HAVE_QUORUM_RE.match(line)
+    if m:
+        res_name, have_quorum = m.groups()
+        if res_name in resources and have_quorum == 'no':
+            systemd_units_str = resources[res_name]['systemd-units']
+            systemd_units = get_systemd_units(systemd_units_str)
+            to_stop = [x for x in systemd_units if x in services]
+            if to_stop:
+                print('Lost quorum on %s' % (res_name))
+            for systemd_unit in reversed(to_stop):
+                r = call_systemd('stop', systemd_unit)
+                if r:
+                    services.remove(systemd_unit)
+
+
+def active_drbd_volume(res_name):
+    retry = True
+    args = ['drbdadm', 'adjust', res_name]
+    while True:
+        proc = subprocess.Popen(args, stderr=subprocess.PIPE)
+        (stdout, stderr) = proc.communicate()
+        if not proc.returncode:
+            return  # Success. \o/
+
+        if not retry:
+            break
+
+        m = DRBDADM_OPEN_FAILED_RE.match(stderr)
+        if m and subprocess.call(['lvchange', '-ay', m.groups()[0]]) == 0:
+            retry = False
+        else:
+            break
+
+    print('Failed to execute `{}`: {}'.format(args, stderr))
+
+
+def main():
+    services = []
+    status = dict()
+    config = configparser.ConfigParser()
+    config.read('/etc/minidrbdcluster.ini')
+    resources = config._sections
+    if not resources:
+        raise Exception(
+            'No resources to watch, maybe /etc/minidrbdcluster.ini missing'
+        )
+    print('Managing DRBD resources: %s' % (' '.join(resources)))
+    for res_name in resources:
+        status[res_name] = dict()
+        active_drbd_volume(res_name)
+
+    signal.signal(signal.SIGHUP, sig_handler)
+
+    print('Starting process...')
+    events2 = subprocess.Popen(
+        ['drbdsetup', 'events2'], stdout=subprocess.PIPE
+    )
+    run = True
+    while run:
+        try:
+            process(events2, resources, services, status)
+        except KeyboardInterrupt:
+            run = False
+        except SigHupException:
+            show_status(services, status)
+
+    clean_up(services)
+
+
+if __name__ == '__main__':
+    main()
diff --git a/systemd/minidrbdcluster.service b/systemd/minidrbdcluster.service
new file mode 100644
index 00000000..3de6ac4f
--- /dev/null
+++ b/systemd/minidrbdcluster.service
@@ -0,0 +1,18 @@
+[Unit]
+Description=Minimalistic high-availability cluster resource manager
+Before=xs-sm.service
+Wants=network-online.target
+After=network-online.target
+
+[Service]
+Type=simple
+Environment=PYTHONUNBUFFERED=1
+ExecStart=/opt/xensource/libexec/minidrbdcluster
+KillMode=process
+KillSignal=SIGINT
+StandardOutput=journal
+StandardError=journal
+SyslogIdentifier=minidrbdcluster
+
+[Install]
+WantedBy=multi-user.target
