From 918ebd8d01ca76bd5f0523a7fe567c105fe33cd1 Mon Sep 17 00:00:00 2001
From: Wescoeur <ronan.abhamon@vates.fr>
Date: Thu, 25 Feb 2021 17:52:57 +0100
Subject: [PATCH 033/162] feat(LinstorSR): protect sr commands to avoid
 forgetting LINSTOR volumes when master satellite is down

Steps to reproduce:

- Ensure the linstor satellite is not running on the master host, otherwise stop it
- Then restart the controller on the right host where the LINSTOR database is mounted
- Run st_attach command => All volumes will be forgotten

To avoid this, it's possible to restart the satellite on the master before the sr_attach command.
Also it's funny to see you can start and stop the satellite juste before the sr_attach, and the volumes will not be removed.

Explanations:

In theory this bug is impossible because during the sr_attach execution, an exception is thrown
(so sr_scan should not be executed) BUT there is a piece of code that is executed
in SRCommand.py when sr_attach is called:

```python
try:
    return sr.attach(sr_uuid)
finally:
    if is_master:
        sr.after_master_attach(sr_uuid)
```

The exception is not immediately forwarded because a finally block must be executed before.
And what is the implementation of after_master_attach?

```python
def after_master_attach(self, uuid):
    """Perform actions required after attaching on the pool master
    Return:
    None
    """
    self.scan(uuid)
```

Oh! Of course, a scan is always executed after a attach... What's the purpose of a scan if we can't
execute correctly an attach command before? I don't know, but it's probably error-prone like this context.
When scan is called, we suppose the SR is attached and we have all VDIs loaded but it's not the case
because an exception has been thrown.

To solve this problem we forbid the execution of the scan if the attach failed.

Signed-off-by: Ronan Abhamon <ronan.abhamon@vates.fr>
---
 drivers/LinstorSR.py | 47 ++++++++++++++++++++++++++++++++++----------
 1 file changed, 37 insertions(+), 10 deletions(-)

diff --git a/drivers/LinstorSR.py b/drivers/LinstorSR.py
index d943d49..092f5e8 100755
--- a/drivers/LinstorSR.py
+++ b/drivers/LinstorSR.py
@@ -256,6 +256,11 @@ class LinstorSR(SR.SR):
 
     MANAGER_PLUGIN = 'linstor-manager'
 
+    INIT_STATUS_NOT_SET = 0
+    INIT_STATUS_IN_PROGRESS = 1
+    INIT_STATUS_OK = 2
+    INIT_STATUS_FAIL = 3
+
     # --------------------------------------------------------------------------
     # SR methods.
     # --------------------------------------------------------------------------
@@ -325,19 +330,18 @@ class LinstorSR(SR.SR):
 
         self._vdi_shared_time = 0
 
-        self._initialized = False
+        self._init_status = self.INIT_STATUS_NOT_SET
 
         self._vdis_loaded = False
         self._all_volume_info_cache = None
         self._all_volume_metadata_cache = None
 
     def _locked_load(method):
-        @functools.wraps(method)
-        def wrap(self, *args, **kwargs):
-            if self._initialized:
-                return method(self, *args, **kwargs)
-            self._initialized = True
+        def wrapped_method(self, *args, **kwargs):
+            self._init_status = self.INIT_STATUS_OK
+            return method(self, *args, **kwargs)
 
+        def load(self, *args, **kwargs):
             if not self._has_session:
                 if self.srcmd.cmd == 'vdi_attach_from_config':
                     # We must have a valid LINSTOR instance here without using
@@ -352,7 +356,7 @@ class LinstorSR(SR.SR):
                         self._group_name,
                         logger=util.SMlog
                     )
-                return method(self, *args, **kwargs)
+                return wrapped_method(self, *args, **kwargs)
 
             if not self._is_master:
                 if self.cmd in [
@@ -456,11 +460,29 @@ class LinstorSR(SR.SR):
                     )
                     util.SMlog(traceback.format_exc())
 
-            return method(self, *args, **kwargs)
+            return wrapped_method(self, *args, **kwargs)
+
+        @functools.wraps(wrapped_method)
+        def wrap(self, *args, **kwargs):
+            if self._init_status in \
+                    (self.INIT_STATUS_OK, self.INIT_STATUS_IN_PROGRESS):
+                return wrapped_method(self, *args, **kwargs)
+            if self._init_status == self.INIT_STATUS_FAIL:
+                util.SMlog(
+                    'Can\'t call method {} because initialization failed'
+                    .format(method)
+                )
+            else:
+                try:
+                    self._init_status = self.INIT_STATUS_IN_PROGRESS
+                    return load(self, *args, **kwargs)
+                except Exception:
+                    if self._init_status != self.INIT_STATUS_OK:
+                        self._init_status = self.INIT_STATUS_FAIL
+                    raise
 
         return wrap
 
-    @_locked_load
     def cleanup(self):
         if self._vdi_shared_time:
             self._shared_lock_vdi(self.srcmd.params['vdi_uuid'], locked=False)
@@ -657,6 +679,9 @@ class LinstorSR(SR.SR):
 
     @_locked_load
     def scan(self, uuid):
+        if self._init_status == self.INIT_STATUS_FAIL:
+            return
+
         util.SMlog('LinstorSR.scan for {}'.format(self.uuid))
         if not self._linstor:
             raise xs_errors.XenError(
@@ -855,7 +880,6 @@ class LinstorSR(SR.SR):
     def _load_vdis(self):
         if self._vdis_loaded:
             return
-        self._vdis_loaded = True
 
         assert self._is_master
 
@@ -866,6 +890,9 @@ class LinstorSR(SR.SR):
         self._load_vdis_ex()
         self._destroy_linstor_cache()
 
+        # We must mark VDIs as loaded only if the load is a success.
+        self._vdis_loaded = True
+
         self._undo_all_journal_transactions()
 
     def _load_vdis_ex(self):
-- 
2.43.0

