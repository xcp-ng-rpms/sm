From 97baadcc87c24601e6eb7cde21e5f4306d7ffd77 Mon Sep 17 00:00:00 2001
From: Ronan Abhamon <ronan.abhamon@vates.fr>
Date: Fri, 8 Jan 2021 16:12:15 +0100
Subject: [PATCH 029/171] feat(LinstorSR): display a correctly readable size
 for the user

Signed-off-by: Ronan Abhamon <ronan.abhamon@vates.fr>
---
 drivers/LinstorSR.py            | 46 +++++++++------------
 drivers/linstorvolumemanager.py | 72 +++++++++++++++++++++++++++++----
 2 files changed, 83 insertions(+), 35 deletions(-)

diff --git a/drivers/LinstorSR.py b/drivers/LinstorSR.py
index 52131a5..16cb0d6 100755
--- a/drivers/LinstorSR.py
+++ b/drivers/LinstorSR.py
@@ -770,27 +770,19 @@ class LinstorSR(SR.SR):
         # Update size attributes of the SR parent class.
         self.virtual_allocation = valloc + virt_alloc_delta
 
-        # Physical size contains the total physical size.
-        # i.e. the sum of the sizes of all devices on all hosts, not the AVG.
         self._update_physical_size()
 
         # Notify SR parent class.
         self._db_update()
 
     def _update_physical_size(self):
-        # Physical size contains the total physical size.
-        # i.e. the sum of the sizes of all devices on all hosts, not the AVG.
-        self.physical_size = self._linstor.physical_size
+        # We use the size of the smallest disk, this is an approximation that
+        # ensures the displayed physical size is reachable by the user.
+        self.physical_size = \
+            self._linstor.min_physical_size * len(self._hosts) / \
+            self._redundancy
 
-        # `self._linstor.physical_free_size` contains the total physical free
-        # memory. If Thin provisioning is used we can't use it, we must use
-        # LINSTOR volume size to gives a good idea of the required
-        # usable memory to the users.
-        self.physical_utilisation = self._linstor.total_allocated_volume_size
-
-        # If Thick provisioning is used, we can use this line instead:
-        # self.physical_utilisation = \
-        #     self.physical_size - self._linstor.physical_free_size
+        self.physical_utilisation = self._linstor.allocated_volume_size
 
     # --------------------------------------------------------------------------
     # VDIs.
@@ -912,10 +904,10 @@ class LinstorSR(SR.SR):
 
                 util.SMlog(
                     'Introducing VDI {} '.format(vdi_uuid) +
-                    ' (name={}, virtual_size={}, physical_size={})'.format(
+                    ' (name={}, virtual_size={}, allocated_size={})'.format(
                         name_label,
                         volume_info.virtual_size,
-                        volume_info.physical_size
+                        volume_info.allocated_size
                     )
                 )
 
@@ -933,7 +925,7 @@ class LinstorSR(SR.SR):
                     sm_config,
                     managed,
                     str(volume_info.virtual_size),
-                    str(volume_info.physical_size)
+                    str(volume_info.allocated_size)
                 )
 
                 is_a_snapshot = volume_metadata.get(IS_A_SNAPSHOT_TAG)
@@ -1016,7 +1008,7 @@ class LinstorSR(SR.SR):
                 else:
                     geneology[vdi.parent] = [vdi_uuid]
             if not vdi.hidden:
-                self.virtual_allocation += vdi.utilisation
+                self.virtual_allocation += vdi.size
 
         # 9. Remove all hidden leaf nodes to avoid introducing records that
         # will be GC'ed.
@@ -1453,11 +1445,11 @@ class LinstorVDI(VDI.VDI):
                         '{}'.format(e)
                     )
 
-        self.utilisation = volume_info.physical_size
+        self.utilisation = volume_info.allocated_size
         self.sm_config['vdi_type'] = self.vdi_type
 
         self.ref = self._db_introduce()
-        self.sr._update_stats(volume_info.virtual_size)
+        self.sr._update_stats(self.size)
 
         return VDI.VDI.get_params(self)
 
@@ -1496,7 +1488,7 @@ class LinstorVDI(VDI.VDI):
             del self.sr.vdis[self.uuid]
 
         # TODO: Check size after delete.
-        self.sr._update_stats(-self.capacity)
+        self.sr._update_stats(-self.size)
         self.sr._kick_gc()
         return super(LinstorVDI, self).delete(sr_uuid, vdi_uuid, data_only)
 
@@ -1622,7 +1614,7 @@ class LinstorVDI(VDI.VDI):
         space_needed = new_volume_size - old_volume_size
         self.sr._ensure_space_available(space_needed)
 
-        old_capacity = self.capacity
+        old_size = self.size
         if self.vdi_type == vhdutil.VDI_TYPE_RAW:
             self._linstor.resize(self.uuid, new_volume_size)
         else:
@@ -1641,7 +1633,7 @@ class LinstorVDI(VDI.VDI):
         self.session.xenapi.VDI.set_physical_utilisation(
             vdi_ref, str(self.utilisation)
         )
-        self.sr._update_stats(self.capacity - old_capacity)
+        self.sr._update_stats(self.size - old_size)
         return VDI.VDI.get_params(self)
 
     def clone(self, sr_uuid, vdi_uuid):
@@ -1756,13 +1748,13 @@ class LinstorVDI(VDI.VDI):
         if volume_info is None:
             volume_info = self._linstor.get_volume_info(self.uuid)
 
-        # Contains the physical size used on all disks.
+        # Contains the max physical size used on a disk.
         # When LINSTOR LVM driver is used, the size should be similar to
         # virtual size (i.e. the LINSTOR max volume size).
         # When LINSTOR Thin LVM driver is used, the used physical size should
         # be lower than virtual size at creation.
         # The physical size increases after each write in a new block.
-        self.utilisation = volume_info.physical_size
+        self.utilisation = volume_info.allocated_size
         self.capacity = volume_info.virtual_size
 
         if self.vdi_type == vhdutil.VDI_TYPE_RAW:
@@ -1958,7 +1950,7 @@ class LinstorVDI(VDI.VDI):
         volume_info = self._linstor.get_volume_info(snap_uuid)
 
         snap_vdi.size = self.sr._vhdutil.get_size_virt(snap_uuid)
-        snap_vdi.utilisation = volume_info.physical_size
+        snap_vdi.utilisation = volume_info.allocated_size
 
         # 6. Update sm config.
         snap_vdi.sm_config = {}
@@ -2156,7 +2148,7 @@ class LinstorVDI(VDI.VDI):
                     raise
 
             if snap_type != VDI.SNAPSHOT_INTERNAL:
-                self.sr._update_stats(self.capacity)
+                self.sr._update_stats(self.size)
 
             # 10. Return info on the new user-visible leaf VDI.
             ret_vdi = snap_vdi
diff --git a/drivers/linstorvolumemanager.py b/drivers/linstorvolumemanager.py
index d617655..a6f67d8 100755
--- a/drivers/linstorvolumemanager.py
+++ b/drivers/linstorvolumemanager.py
@@ -131,20 +131,19 @@ class LinstorVolumeManager(object):
     class VolumeInfo(object):
         __slots__ = (
             'name',
-            'physical_size',  # Total physical size used by this volume on
-                              # all disks.
+            'allocated_size',  # Allocated size, place count is not used.
             'virtual_size'    # Total virtual available size of this volume
                               # (i.e. the user size at creation).
         )
 
         def __init__(self, name):
             self.name = name
-            self.physical_size = 0
+            self.allocated_size = 0
             self.virtual_size = 0
 
         def __repr__(self):
             return 'VolumeInfo("{}", {}, {})'.format(
-                self.name, self.physical_size, self.virtual_size
+                self.name, self.allocated_size, self.virtual_size
             )
 
     # --------------------------------------------------------------------------
@@ -248,9 +247,31 @@ class LinstorVolumeManager(object):
         return self._compute_size('free_capacity')
 
     @property
-    def total_allocated_volume_size(self):
+    def min_physical_size(self):
         """
-        Give the sum of all created volumes.
+        Give the minimum physical size of the SR.
+        I.e. the size of the smallest disk.
+        :return: The physical min size.
+        :rtype: int
+        """
+        size = None
+        for pool in self._get_storage_pools(force=True):
+            space = pool.free_space
+            if space:
+                current_size = space.total_capacity
+                if current_size < 0:
+                    raise LinstorVolumeManagerError(
+                        'Failed to get pool total_capacity attr of `{}`'
+                        .format(pool.node_name)
+                    )
+                if size is None or current_size < size:
+                    size = current_size
+        return size * 1024
+
+    @property
+    def total_volume_size(self):
+        """
+        Give the sum of all created volumes. The place count is used.
         :return: The physical required size to use the volumes.
         :rtype: int
         """
@@ -269,6 +290,37 @@ class LinstorVolumeManager(object):
                     size += current_size
         return size * 1024
 
+    @property
+    def allocated_volume_size(self):
+        """
+        Give the allocated size for all volumes. The place count is not
+        used here. When thick lvm is used, the size for one volume should
+        be equal to the virtual volume size. With thin lvm, the size is equal
+        or lower to the volume size.
+        :return: The allocated size of all volumes.
+        :rtype: int
+        """
+
+        size = 0
+        for resource in self._get_resource_cache().resources:
+            volume_size = None
+            for volume in resource.volumes:
+                # We ignore diskless pools of the form "DfltDisklessStorPool".
+                if volume.storage_pool_name == self._group_name:
+                    current_size = volume.allocated_size
+                    if current_size < 0:
+                        raise LinstorVolumeManagerError(
+                           'Failed to get allocated size of `{}` on `{}`'
+                           .format(resource.name, volume.storage_pool_name)
+                        )
+
+                    if volume_size is None or current_size > volume_size:
+                        volume_size = current_size
+            if volume_size is not None:
+                size += volume_size
+
+        return size * 1024
+
     @property
     def metadata(self):
         """
@@ -1328,7 +1380,11 @@ class LinstorVolumeManager(object):
                            'Failed to get allocated size of `{}` on `{}`'
                            .format(resource.name, volume.storage_pool_name)
                         )
-                    current.physical_size += volume.allocated_size
+                    allocated_size = volume.allocated_size
+
+                    current.allocated_size = current.allocated_size and \
+                        max(current.allocated_size, allocated_size) or \
+                        allocated_size
 
                     if volume.usable_size < 0:
                         raise LinstorVolumeManagerError(
@@ -1341,7 +1397,7 @@ class LinstorVolumeManager(object):
                         min(current.virtual_size, virtual_size) or virtual_size
 
         for current in all_volume_info.values():
-            current.physical_size *= 1024
+            current.allocated_size *= 1024
             current.virtual_size *= 1024
 
         self._volume_info_cache_dirty = False
-- 
2.44.0

