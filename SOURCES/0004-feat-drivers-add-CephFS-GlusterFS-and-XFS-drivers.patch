From 21ad2a6035dca5e2dc889880a90020348be52dd8 Mon Sep 17 00:00:00 2001
From: Ronan Abhamon <ronan.abhamon@vates.fr>
Date: Mon, 20 Jul 2020 16:26:42 +0200
Subject: [PATCH 04/16] feat(drivers): add CephFS, GlusterFS and XFS drivers

---
 Makefile               |   3 +
 drivers/CephFSSR.py    | 296 +++++++++++++++++++++++++++++++++++++++++
 drivers/GlusterFSSR.py | 287 +++++++++++++++++++++++++++++++++++++++
 drivers/XFSSR.py       | 242 +++++++++++++++++++++++++++++++++
 drivers/cleanup.py     |   5 +-
 5 files changed, 832 insertions(+), 1 deletion(-)
 create mode 100644 drivers/CephFSSR.py
 create mode 100644 drivers/GlusterFSSR.py
 create mode 100644 drivers/XFSSR.py

diff --git a/Makefile b/Makefile
index f752a9e..8f06e91 100755
--- a/Makefile
+++ b/Makefile
@@ -17,6 +17,9 @@ SM_DRIVERS += OCFSoHBA
 SM_DRIVERS += SHM
 SM_DRIVERS += SMB
 SM_DRIVERS += LVHDoFCoE
+SM_DRIVERS += CephFS
+SM_DRIVERS += GlusterFS
+SM_DRIVERS += XFS
 
 SM_LIBS := SR
 SM_LIBS += SRCommand
diff --git a/drivers/CephFSSR.py b/drivers/CephFSSR.py
new file mode 100644
index 0000000..d797490
--- /dev/null
+++ b/drivers/CephFSSR.py
@@ -0,0 +1,296 @@
+#!/usr/bin/env python
+#
+# Original work copyright (C) Citrix systems
+# Modified work copyright (C) Vates SAS and XCP-ng community
+#
+# This program is free software; you can redistribute it and/or modify
+# it under the terms of the GNU Lesser General Public License as published
+# by the Free Software Foundation; version 2.1 only.
+#
+# This program is distributed in the hope that it will be useful,
+# but WITHOUT ANY WARRANTY; without even the implied warranty of
+# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
+# GNU Lesser General Public License for more details.
+#
+# You should have received a copy of the GNU Lesser General Public License
+# along with this program; if not, write to the Free Software Foundation, Inc.,
+# 51 Franklin Street, Fifth Floor, Boston, MA  02110-1301  USA
+#
+# CEPHFSSR: Based on FileSR, mounts ceph fs share
+
+import errno
+import os
+import syslog as _syslog
+import xmlrpclib
+from syslog import syslog
+
+# careful with the import order here
+# FileSR has a circular dependency:
+# FileSR -> blktap2 -> lvutil -> EXTSR -> FileSR
+# importing in this order seems to avoid triggering the issue.
+import SR
+import SRCommand
+import FileSR
+# end of careful
+import cleanup
+import util
+import vhdutil
+import xs_errors
+from lock import Lock
+
+CAPABILITIES = ["SR_PROBE", "SR_UPDATE", "SR_CACHING",
+                "VDI_CREATE", "VDI_DELETE", "VDI_ATTACH", "VDI_DETACH",
+                "VDI_UPDATE", "VDI_CLONE", "VDI_SNAPSHOT", "VDI_RESIZE", "VDI_MIRROR",
+                "VDI_GENERATE_CONFIG",
+                "VDI_RESET_ON_BOOT/2", "ATOMIC_PAUSE"]
+
+CONFIGURATION = [
+    ['server', 'Ceph server(s) (required, ex: "192.168.0.12" or "10.10.10.10,10.10.10.26")'],
+    ['serverpath', 'Ceph FS path (required, ex: "/")'],
+    ['serverport', 'ex: 6789'],
+    ['options', 'Ceph FS client name, and secretfile (required, ex: "name=admin,secretfile=/etc/ceph/admin.secret")']
+]
+
+DRIVER_INFO = {
+    'name': 'CephFS VHD',
+    'description': 'SR plugin which stores disks as VHD files on a CephFS storage',
+    'vendor': 'Vates SAS',
+    'copyright': '(C) 2020 Vates SAS',
+    'driver_version': '1.0',
+    'required_api_version': '1.0',
+    'capabilities': CAPABILITIES,
+    'configuration': CONFIGURATION
+}
+
+DRIVER_CONFIG = {"ATTACH_FROM_CONFIG_WITH_TAPDISK": True}
+
+# The mountpoint for the directory when performing an sr_probe.  All probes
+# are guaranteed to be serialised by xapi, so this single mountpoint is fine.
+PROBE_MOUNTPOINT = os.path.join(SR.MOUNT_BASE, "probe")
+
+
+class CephFSException(Exception):
+    def __init__(self, errstr):
+        self.errstr = errstr
+
+
+# mountpoint = /var/run/sr-mount/CephFS/uuid
+# linkpath = mountpoint/uuid - path to SR directory on share
+# path = /var/run/sr-mount/uuid - symlink to SR directory on share
+class CephFSSR(FileSR.FileSR):
+    """Ceph file-based storage repository"""
+
+    DRIVER_TYPE = 'cephfs'
+
+    def handles(sr_type):
+        # fudge, because the parent class (FileSR) checks for smb to alter its behavior
+        return sr_type == CephFSSR.DRIVER_TYPE or sr_type == 'smb'
+
+    handles = staticmethod(handles)
+
+    def load(self, sr_uuid):
+        if not self._is_ceph_available():
+            raise xs_errors.XenError(
+                'SRUnavailable',
+                opterr='ceph is not installed'
+            )
+
+        self.ops_exclusive = FileSR.OPS_EXCLUSIVE
+        self.lock = Lock(vhdutil.LOCK_TYPE_SR, self.uuid)
+        self.sr_vditype = SR.DEFAULT_TAP
+        self.driver_config = DRIVER_CONFIG
+        if 'server' not in self.dconf:
+            raise xs_errors.XenError('ConfigServerMissing')
+        self.remoteserver = self.dconf['server']
+        self.remotepath = self.dconf['serverpath']
+        # if serverport is not specified, use default 6789
+        if 'serverport' not in self.dconf:
+            self.remoteport = "6789"
+        else:
+            self.remoteport = self.dconf['serverport']
+        if self.sr_ref and self.session is not None:
+            self.sm_config = self.session.xenapi.SR.get_sm_config(self.sr_ref)
+        else:
+            self.sm_config = self.srcmd.params.get('sr_sm_config') or {}
+        self.mountpoint = os.path.join(SR.MOUNT_BASE, 'CephFS', sr_uuid)
+        self.linkpath = os.path.join(self.mountpoint, sr_uuid or "")
+        self.path = os.path.join(SR.MOUNT_BASE, sr_uuid)
+        self._check_o_direct()
+
+    def checkmount(self):
+        return util.ioretry(lambda: ((util.pathexists(self.mountpoint) and
+                                      util.ismount(self.mountpoint)) and
+                                     util.pathexists(self.path)))
+
+    def mount(self, mountpoint=None):
+        """Mount the remote ceph export at 'mountpoint'"""
+        if mountpoint is None:
+            mountpoint = self.mountpoint
+        elif not util.is_string(mountpoint) or mountpoint == "":
+            raise CephFSException("mountpoint not a string object")
+
+        try:
+            if not util.ioretry(lambda: util.isdir(mountpoint)):
+                util.ioretry(lambda: util.makedirs(mountpoint))
+        except util.CommandException, inst:
+            raise CephFSException("Failed to make directory: code is %d" % inst.code)
+
+        try:
+            options = []
+            if self.dconf.has_key('options'):
+                options.append(self.dconf['options'])
+            if options:
+                options = ['-o', ','.join(options)]
+            command = ["mount", '-t', 'ceph', self.remoteserver+":"+self.remoteport+":"+self.remotepath, mountpoint] + options
+            util.ioretry(lambda: util.pread(command), errlist=[errno.EPIPE, errno.EIO], maxretry=2, nofail=True)
+        except util.CommandException, inst:
+            syslog(_syslog.LOG_ERR, 'CephFS mount failed ' + inst.__str__())
+            raise CephFSException("mount failed with return code %d" % inst.code)
+
+        # Sanity check to ensure that the user has at least RO access to the
+        # mounted share. Windows sharing and security settings can be tricky.
+        try:
+            util.listdir(mountpoint)
+        except util.CommandException:
+            try:
+                self.unmount(mountpoint, True)
+            except CephFSException:
+                util.logException('CephFSSR.unmount()')
+            raise CephFSException("Permission denied. Please check user privileges.")
+
+    def unmount(self, mountpoint, rmmountpoint):
+        try:
+            util.pread(["umount", mountpoint])
+        except util.CommandException, inst:
+            raise CephFSException("umount failed with return code %d" % inst.code)
+        if rmmountpoint:
+            try:
+                os.rmdir(mountpoint)
+            except OSError, inst:
+                raise CephFSException("rmdir failed with error '%s'" % inst.strerror)
+
+    def attach(self, sr_uuid):
+        if not self.checkmount():
+            try:
+                self.mount()
+                os.symlink(self.linkpath, self.path)
+            except CephFSException, exc:
+                raise SR.SROSError(12, exc.errstr)
+        self.attached = True
+
+    def probe(self):
+        try:
+            self.mount(PROBE_MOUNTPOINT)
+            sr_list = filter(util.match_uuid, util.listdir(PROBE_MOUNTPOINT))
+            self.unmount(PROBE_MOUNTPOINT, True)
+        except (util.CommandException, xs_errors.XenError):
+            raise
+        # Create a dictionary from the SR uuids to feed SRtoXML()
+        sr_dict = {sr_uuid: {} for sr_uuid in sr_list}
+        return util.SRtoXML(sr_dict)
+
+    def detach(self, sr_uuid):
+        if not self.checkmount():
+            return
+        util.SMlog("Aborting GC/coalesce")
+        cleanup.abort(self.uuid)
+        # Change directory to avoid unmount conflicts
+        os.chdir(SR.MOUNT_BASE)
+        self.unmount(self.mountpoint, True)
+        os.unlink(self.path)
+        self.attached = False
+
+    def create(self, sr_uuid, size):
+        if self.checkmount():
+            raise SR.SROSError(113, 'CephFS mount point already attached')
+
+        try:
+            self.mount()
+        except CephFSException, exc:
+            # noinspection PyBroadException
+            try:
+                os.rmdir(self.mountpoint)
+            except:
+                # we have no recovery strategy
+                pass
+            raise SR.SROSError(111, "CephFS mount error [opterr=%s]" % exc.errstr)
+
+        if util.ioretry(lambda: util.pathexists(self.linkpath)):
+            if len(util.ioretry(lambda: util.listdir(self.linkpath))) != 0:
+                self.detach(sr_uuid)
+                raise xs_errors.XenError('SRExists')
+        else:
+            try:
+                util.ioretry(lambda: util.makedirs(self.linkpath))
+                os.symlink(self.linkpath, self.path)
+            except util.CommandException, inst:
+                if inst.code != errno.EEXIST:
+                    try:
+                        self.unmount(self.mountpoint, True)
+                    except CephFSException:
+                        util.logException('CephFSSR.unmount()')
+                    raise SR.SROSError(116,
+                                       "Failed to create CephFS SR. remote directory creation error: {}".format(
+                                           os.strerror(inst.code)))
+        self.detach(sr_uuid)
+
+    def delete(self, sr_uuid):
+        # try to remove/delete non VDI contents first
+        super(CephFSSR, self).delete(sr_uuid)
+        try:
+            if self.checkmount():
+                self.detach(sr_uuid)
+            self.mount()
+            if util.ioretry(lambda: util.pathexists(self.linkpath)):
+                util.ioretry(lambda: os.rmdir(self.linkpath))
+            util.SMlog(str(self.unmount(self.mountpoint, True)))
+        except util.CommandException, inst:
+            self.detach(sr_uuid)
+            if inst.code != errno.ENOENT:
+                raise SR.SROSError(114, "Failed to remove CephFS mount point")
+
+    def vdi(self, uuid, loadLocked=False):
+        return CephFSFileVDI(self, uuid)
+
+    @staticmethod
+    def _is_ceph_available():
+        import distutils.spawn
+        return distutils.spawn.find_executable('ceph')
+
+class CephFSFileVDI(FileSR.FileVDI):
+    def attach(self, sr_uuid, vdi_uuid):
+        if not hasattr(self, 'xenstore_data'):
+            self.xenstore_data = {}
+
+        self.xenstore_data['storage-type'] = CephFSSR.DRIVER_TYPE
+
+        return super(CephFSFileVDI, self).attach(sr_uuid, vdi_uuid)
+
+    def generate_config(self, sr_uuid, vdi_uuid):
+        util.SMlog("SMBFileVDI.generate_config")
+        if not util.pathexists(self.path):
+            raise xs_errors.XenError('VDIUnavailable')
+        resp = {'device_config': self.sr.dconf,
+                'sr_uuid': sr_uuid,
+                'vdi_uuid': vdi_uuid,
+                'sr_sm_config': self.sr.sm_config,
+                'command': 'vdi_attach_from_config'}
+        # Return the 'config' encoded within a normal XMLRPC response so that
+        # we can use the regular response/error parsing code.
+        config = xmlrpclib.dumps(tuple([resp]), "vdi_attach_from_config")
+        return xmlrpclib.dumps((config,), "", True)
+
+    def attach_from_config(self, sr_uuid, vdi_uuid):
+        try:
+            if not util.pathexists(self.sr.path):
+                self.sr.attach(sr_uuid)
+        except:
+            util.logException("SMBFileVDI.attach_from_config")
+            raise xs_errors.XenError('SRUnavailable',
+                                     opterr='Unable to attach from config')
+
+
+if __name__ == '__main__':
+    SRCommand.run(CephFSSR, DRIVER_INFO)
+else:
+    SR.registerSR(CephFSSR)
diff --git a/drivers/GlusterFSSR.py b/drivers/GlusterFSSR.py
new file mode 100644
index 0000000..a2f7484
--- /dev/null
+++ b/drivers/GlusterFSSR.py
@@ -0,0 +1,287 @@
+#!/usr/bin/env python
+#
+# Original work copyright (C) Citrix systems
+# Modified work copyright (C) Vates SAS and XCP-ng community
+#
+# This program is free software; you can redistribute it and/or modify
+# it under the terms of the GNU Lesser General Public License as published
+# by the Free Software Foundation; version 2.1 only.
+#
+# This program is distributed in the hope that it will be useful,
+# but WITHOUT ANY WARRANTY; without even the implied warranty of
+# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
+# GNU Lesser General Public License for more details.
+#
+# You should have received a copy of the GNU Lesser General Public License
+# along with this program; if not, write to the Free Software Foundation, Inc.,
+# 51 Franklin Street, Fifth Floor, Boston, MA  02110-1301  USA
+
+import errno
+import os
+import syslog as _syslog
+import xmlrpclib
+from syslog import syslog
+
+# careful with the import order here
+# FileSR has a circular dependency: FileSR- > blktap2 -> lvutil -> EXTSR -> FileSR
+# importing in this order seems to avoid triggering the issue.
+import SR
+import SRCommand
+import FileSR
+# end of careful
+import cleanup
+import util
+import vhdutil
+import xs_errors
+from lock import Lock
+
+CAPABILITIES = ["SR_PROBE", "SR_UPDATE", "SR_CACHING",
+                "VDI_CREATE", "VDI_DELETE", "VDI_ATTACH", "VDI_DETACH",
+                "VDI_UPDATE", "VDI_CLONE", "VDI_SNAPSHOT", "VDI_RESIZE", "VDI_MIRROR",
+                "VDI_GENERATE_CONFIG",
+                "VDI_RESET_ON_BOOT/2", "ATOMIC_PAUSE"]
+
+CONFIGURATION = [['server', 'Full path to share on gluster server (required, ex: "192.168.0.12:/gv0")'],
+                 ['backupservers', 'list of servers separated by ":"'],
+                 ['fetchattempts', 'number of attempts to fetch files before switching to the backup server']
+                 ]
+
+DRIVER_INFO = {
+    'name': 'GlusterFS VHD',
+    'description': 'SR plugin which stores disks as VHD files on a GlusterFS storage',
+    'vendor': 'Vates SAS',
+    'copyright': '(C) 2020 Vates SAS',
+    'driver_version': '1.0',
+    'required_api_version': '1.0',
+    'capabilities': CAPABILITIES,
+    'configuration': CONFIGURATION
+}
+
+DRIVER_CONFIG = {"ATTACH_FROM_CONFIG_WITH_TAPDISK": True}
+
+# The mountpoint for the directory when performing an sr_probe.  All probes
+# are guaranteed to be serialised by xapi, so this single mountpoint is fine.
+PROBE_MOUNTPOINT = os.path.join(SR.MOUNT_BASE, "probe")
+
+
+class GlusterFSException(Exception):
+    def __init__(self, errstr):
+        self.errstr = errstr
+
+
+# mountpoint = /var/run/sr-mount/GlusterFS/<glusterfs_server_name>/uuid
+# linkpath = mountpoint/uuid - path to SR directory on share
+# path = /var/run/sr-mount/uuid - symlink to SR directory on share
+class GlusterFSSR(FileSR.FileSR):
+    """Gluster file-based storage repository"""
+
+    DRIVER_TYPE = 'glusterfs'
+
+    def handles(sr_type):
+        # fudge, because the parent class (FileSR) checks for smb to alter its behavior
+        return sr_type == GlusterFSSR.DRIVER_TYPE or sr_type == 'smb'
+
+    handles = staticmethod(handles)
+
+    def load(self, sr_uuid):
+        if not self._is_glusterfs_available():
+            raise xs_errors.XenError(
+                'SRUnavailable',
+                opterr='glusterfs is not installed'
+            )
+
+        self.ops_exclusive = FileSR.OPS_EXCLUSIVE
+        self.lock = Lock(vhdutil.LOCK_TYPE_SR, self.uuid)
+        self.sr_vditype = SR.DEFAULT_TAP
+        self.driver_config = DRIVER_CONFIG
+        if 'server' not in self.dconf:
+            raise xs_errors.XenError('ConfigServerMissing')
+        self.remoteserver = self.dconf['server']
+        if self.sr_ref and self.session is not None:
+            self.sm_config = self.session.xenapi.SR.get_sm_config(self.sr_ref)
+        else:
+            self.sm_config = self.srcmd.params.get('sr_sm_config') or {}
+        self.mountpoint = os.path.join(SR.MOUNT_BASE, 'GlusterFS', self.remoteserver.split(':')[0], sr_uuid)
+        self.linkpath = os.path.join(self.mountpoint, sr_uuid or "")
+        self.path = os.path.join(SR.MOUNT_BASE, sr_uuid)
+        self._check_o_direct()
+
+    def checkmount(self):
+        return util.ioretry(lambda: ((util.pathexists(self.mountpoint) and
+                                      util.ismount(self.mountpoint)) and
+                                     util.pathexists(self.linkpath)))
+
+    def mount(self, mountpoint=None):
+        """Mount the remote gluster export at 'mountpoint'"""
+        if mountpoint is None:
+            mountpoint = self.mountpoint
+        elif not util.is_string(mountpoint) or mountpoint == "":
+            raise GlusterFSException("mountpoint not a string object")
+
+        try:
+            if not util.ioretry(lambda: util.isdir(mountpoint)):
+                util.ioretry(lambda: util.makedirs(mountpoint))
+        except util.CommandException, inst:
+            raise GlusterFSException("Failed to make directory: code is %d" % inst.code)
+        try:
+            options = []
+            if 'backupservers' in self.dconf:
+                options.append('backup-volfile-servers=' + self.dconf['backupservers'])
+            if 'fetchattempts' in self.dconf:
+                options.append('fetch-attempts=' + self.dconf['fetchattempts'])
+            if options:
+                options = ['-o', ','.join(options)]
+            command = ["mount", '-t', 'glusterfs', self.remoteserver, mountpoint] + options
+            util.ioretry(lambda: util.pread(command), errlist=[errno.EPIPE, errno.EIO], maxretry=2, nofail=True)
+        except util.CommandException, inst:
+            syslog(_syslog.LOG_ERR, 'GlusterFS mount failed ' + inst.__str__())
+            raise GlusterFSException("mount failed with return code %d" % inst.code)
+
+        # Sanity check to ensure that the user has at least RO access to the
+        # mounted share. Windows sharing and security settings can be tricky.
+        try:
+            util.listdir(mountpoint)
+        except util.CommandException:
+            try:
+                self.unmount(mountpoint, True)
+            except GlusterFSException:
+                util.logException('GlusterFSSR.unmount()')
+            raise GlusterFSException("Permission denied. Please check user privileges.")
+
+    def unmount(self, mountpoint, rmmountpoint):
+        try:
+            util.pread(["umount", mountpoint])
+        except util.CommandException, inst:
+            raise GlusterFSException("umount failed with return code %d" % inst.code)
+        if rmmountpoint:
+            try:
+                os.rmdir(mountpoint)
+            except OSError, inst:
+                raise GlusterFSException("rmdir failed with error '%s'" % inst.strerror)
+
+    def attach(self, sr_uuid):
+        if not self.checkmount():
+            try:
+                self.mount()
+                os.symlink(self.linkpath, self.path)
+            except GlusterFSException, exc:
+                raise SR.SROSError(12, exc.errstr)
+        self.attached = True
+
+    def probe(self):
+        try:
+            self.mount(PROBE_MOUNTPOINT)
+            sr_list = filter(util.match_uuid, util.listdir(PROBE_MOUNTPOINT))
+            self.unmount(PROBE_MOUNTPOINT, True)
+        except (util.CommandException, xs_errors.XenError):
+            raise
+        # Create a dictionary from the SR uuids to feed SRtoXML()
+        sr_dict = {sr_uuid: {} for sr_uuid in sr_list}
+        return util.SRtoXML(sr_dict)
+
+    def detach(self, sr_uuid):
+        if not self.checkmount():
+            return
+        util.SMlog("Aborting GC/coalesce")
+        cleanup.abort(self.uuid)
+        # Change directory to avoid unmount conflicts
+        os.chdir(SR.MOUNT_BASE)
+        self.unmount(self.mountpoint, True)
+        os.unlink(self.path)
+        self.attached = False
+
+    def create(self, sr_uuid, size):
+        if self.checkmount():
+            raise SR.SROSError(113, 'GlusterFS mount point already attached')
+
+        try:
+            self.mount()
+        except GlusterFSException, exc:
+            # noinspection PyBroadException
+            try:
+                os.rmdir(self.mountpoint)
+            except:
+                # we have no recovery strategy
+                pass
+            raise SR.SROSError(111, "GlusterFS mount error [opterr=%s]" % exc.errstr)
+
+        if util.ioretry(lambda: util.pathexists(self.linkpath)):
+            if len(util.ioretry(lambda: util.listdir(self.linkpath))) != 0:
+                self.detach(sr_uuid)
+                raise xs_errors.XenError('SRExists')
+        else:
+            try:
+                util.ioretry(lambda: util.makedirs(self.linkpath))
+                os.symlink(self.linkpath, self.path)
+            except util.CommandException, inst:
+                if inst.code != errno.EEXIST:
+                    try:
+                        self.unmount(self.mountpoint, True)
+                    except GlusterFSException:
+                        util.logException('GlusterFSSR.unmount()')
+                    raise SR.SROSError(116,
+                                       "Failed to create GlusterFS SR. remote directory creation error: {}".format(
+                                           os.strerror(inst.code)))
+        self.detach(sr_uuid)
+
+    def delete(self, sr_uuid):
+        # try to remove/delete non VDI contents first
+        super(GlusterFSSR, self).delete(sr_uuid)
+        try:
+            if self.checkmount():
+                self.detach(sr_uuid)
+            self.mount()
+            if util.ioretry(lambda: util.pathexists(self.linkpath)):
+                util.ioretry(lambda: os.rmdir(self.linkpath))
+            self.unmount(self.mountpoint, True)
+        except util.CommandException, inst:
+            self.detach(sr_uuid)
+            if inst.code != errno.ENOENT:
+                raise SR.SROSError(114, "Failed to remove GlusterFS mount point")
+
+    def vdi(self, uuid, loadLocked=False):
+        return GlusterFSFileVDI(self, uuid)
+
+    @staticmethod
+    def _is_glusterfs_available():
+        import distutils.spawn
+        return distutils.spawn.find_executable('glusterfs')
+
+
+class GlusterFSFileVDI(FileSR.FileVDI):
+    def attach(self, sr_uuid, vdi_uuid):
+        if not hasattr(self, 'xenstore_data'):
+            self.xenstore_data = {}
+
+        self.xenstore_data['storage-type'] = GlusterFSSR.DRIVER_TYPE
+
+        return super(GlusterFSFileVDI, self).attach(sr_uuid, vdi_uuid)
+
+    def generate_config(self, sr_uuid, vdi_uuid):
+        util.SMlog("SMBFileVDI.generate_config")
+        if not util.pathexists(self.path):
+            raise xs_errors.XenError('VDIUnavailable')
+        resp = {'device_config': self.sr.dconf,
+                'sr_uuid': sr_uuid,
+                'vdi_uuid': vdi_uuid,
+                'sr_sm_config': self.sr.sm_config,
+                'command': 'vdi_attach_from_config'}
+        # Return the 'config' encoded within a normal XMLRPC response so that
+        # we can use the regular response/error parsing code.
+        config = xmlrpclib.dumps(tuple([resp]), "vdi_attach_from_config")
+        return xmlrpclib.dumps((config,), "", True)
+
+    def attach_from_config(self, sr_uuid, vdi_uuid):
+        try:
+            if not util.pathexists(self.sr.path):
+                self.sr.attach(sr_uuid)
+        except:
+            util.logException("SMBFileVDI.attach_from_config")
+            raise xs_errors.XenError('SRUnavailable',
+                                     opterr='Unable to attach from config')
+
+
+if __name__ == '__main__':
+    SRCommand.run(GlusterFSSR, DRIVER_INFO)
+else:
+    SR.registerSR(GlusterFSSR)
diff --git a/drivers/XFSSR.py b/drivers/XFSSR.py
new file mode 100644
index 0000000..de35d73
--- /dev/null
+++ b/drivers/XFSSR.py
@@ -0,0 +1,242 @@
+#!/usr/bin/env python
+#
+# Original work copyright (C) Citrix systems
+# Modified work copyright (C) Vates SAS and XCP-ng community
+#
+# This program is free software; you can redistribute it and/or modify
+# it under the terms of the GNU Lesser General Public License as published
+# by the Free Software Foundation; version 2.1 only.
+#
+# This program is distributed in the hope that it will be useful,
+# but WITHOUT ANY WARRANTY; without even the implied warranty of
+# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
+# GNU Lesser General Public License for more details.
+#
+# You should have received a copy of the GNU Lesser General Public License
+# along with this program; if not, write to the Free Software Foundation, Inc.,
+# 51 Franklin Street, Fifth Floor, Boston, MA  02110-1301  USA
+#
+# XFSSR: Based on local-file storage repository, mounts xfs partition
+
+import SR, SRCommand, FileSR, util, lvutil, scsiutil
+
+import os
+import xs_errors
+import vhdutil
+from lock import Lock
+from constants import EXT_PREFIX
+
+CAPABILITIES = ["SR_PROBE","SR_UPDATE", "SR_SUPPORTS_LOCAL_CACHING", \
+                "VDI_CREATE","VDI_DELETE","VDI_ATTACH","VDI_DETACH", \
+                "VDI_UPDATE","VDI_CLONE","VDI_SNAPSHOT","VDI_RESIZE","VDI_MIRROR", \
+                "VDI_GENERATE_CONFIG",                                \
+                "VDI_RESET_ON_BOOT/2","ATOMIC_PAUSE", "VDI_CONFIG_CBT",
+                "VDI_ACTIVATE", "VDI_DEACTIVATE"]
+
+CONFIGURATION = [ [ 'device', 'local device path (required) (e.g. /dev/sda3)' ] ]
+
+DRIVER_INFO = {
+    'name': 'Local XFS VHD',
+    'description': 'SR plugin which represents disks as VHD files stored on a local XFS filesystem, created inside an LVM volume',
+    'vendor': 'Vates SAS',
+    'copyright': '(C) 2019 Vates SAS',
+    'driver_version': '1.0',
+    'required_api_version': '1.0',
+    'capabilities': CAPABILITIES,
+    'configuration': CONFIGURATION
+    }
+
+DRIVER_CONFIG = {"ATTACH_FROM_CONFIG_WITH_TAPDISK": True}
+
+
+class XFSSR(FileSR.FileSR):
+    DRIVER_TYPE = 'xfs'
+
+    """XFS Local file storage repository"""
+    def handles(srtype):
+        return srtype == XFSSR.DRIVER_TYPE
+    handles = staticmethod(handles)
+
+    def load(self, sr_uuid):
+        if not self._is_xfs_available():
+            raise xs_errors.XenError(
+                'SRUnavailable',
+                opterr='xfsprogs is not installed'
+            )
+
+        self.ops_exclusive = FileSR.OPS_EXCLUSIVE
+        self.lock = Lock(vhdutil.LOCK_TYPE_SR, self.uuid)
+        self.sr_vditype = SR.DEFAULT_TAP
+        if not self.dconf.has_key('device') or not self.dconf['device']:
+            raise xs_errors.XenError('ConfigDeviceMissing')
+
+        self.root = self.dconf['device']
+        self.path = os.path.join(SR.MOUNT_BASE, sr_uuid)
+        self.vgname = EXT_PREFIX + sr_uuid
+        self.remotepath = os.path.join("/dev",self.vgname,sr_uuid)
+        self.attached = self._checkmount()
+        self.driver_config = DRIVER_CONFIG
+
+    def delete(self, sr_uuid):
+        super(XFSSR, self).delete(sr_uuid)
+
+        # Check PVs match VG
+        try:
+            for dev in self.root.split(','):
+                cmd = ["pvs", dev]
+                txt = util.pread2(cmd)
+                if txt.find(self.vgname) == -1:
+                    raise xs_errors.XenError('VolNotFound', \
+                          opterr='volume is %s' % self.vgname)
+        except util.CommandException, inst:
+            raise xs_errors.XenError('PVSfailed', \
+                  opterr='error is %d' % inst.code)
+
+        # Remove LV, VG and pv
+        try:
+            cmd = ["lvremove", "-f", self.remotepath]
+            util.pread2(cmd)
+
+            cmd = ["vgremove", self.vgname]
+            util.pread2(cmd)
+
+            for dev in self.root.split(','):
+                cmd = ["pvremove", dev]
+                util.pread2(cmd)
+        except util.CommandException, inst:
+            raise xs_errors.XenError('LVMDelete', \
+                  opterr='errno is %d' % inst.code)
+
+    def attach(self, sr_uuid):
+        if not self._checkmount():
+            try:
+                #Activate LV
+                cmd = ['lvchange','-ay',self.remotepath]
+                util.pread2(cmd)
+
+                # make a mountpoint:
+                if not os.path.isdir(self.path):
+                    os.makedirs(self.path)
+            except util.CommandException, inst:
+                raise xs_errors.XenError('LVMMount', \
+                      opterr='Unable to activate LV. Errno is %d' % inst.code)
+
+            try:
+                util.pread(["fsck", "-a", self.remotepath])
+            except util.CommandException, inst:
+                if inst.code == 1:
+                    util.SMlog("FSCK detected and corrected FS errors. Not fatal.")
+                else:
+                    raise xs_errors.XenError('LVMMount', \
+                         opterr='FSCK failed on %s. Errno is %d' % (self.remotepath,inst.code))
+
+            try:
+                util.pread(["mount", self.remotepath, self.path])
+            except util.CommandException, inst:
+                raise xs_errors.XenError('LVMMount', \
+                      opterr='Failed to mount FS. Errno is %d' % inst.code)
+
+        self.attached = True
+
+        #Update SCSIid string
+        scsiutil.add_serial_record(self.session, self.sr_ref, \
+                scsiutil.devlist_to_serialstring(self.root.split(',')))
+
+        # Set the block scheduler
+        for dev in self.root.split(','): self.block_setscheduler(dev)
+
+    def detach(self, sr_uuid):
+        super(XFSSR, self).detach(sr_uuid)
+        try:
+            # deactivate SR
+            cmd = ["lvchange", "-an", self.remotepath]
+            util.pread2(cmd)
+        except util.CommandException, inst:
+            raise xs_errors.XenError('LVMUnMount', \
+                  opterr='lvm -an failed errno is %d' % inst.code)
+
+    def probe(self):
+        return lvutil.srlist_toxml(lvutil.scan_srlist(EXT_PREFIX, self.root),
+                EXT_PREFIX)
+
+    def create(self, sr_uuid, size):
+        if self._checkmount():
+            raise xs_errors.XenError('SRExists')
+
+        # Check none of the devices already in use by other PBDs
+        if util.test_hostPBD_devs(self.session, sr_uuid, self.root):
+            raise xs_errors.XenError('SRInUse')
+
+        # Check serial number entry in SR records
+        for dev in self.root.split(','):
+            if util.test_scsiserial(self.session, dev):
+                raise xs_errors.XenError('SRInUse')
+
+        if not lvutil._checkVG(self.vgname):
+            lvutil.createVG(self.root, self.vgname)
+
+        if lvutil._checkLV(self.remotepath):
+            raise xs_errors.XenError('SRExists')
+
+        try:
+            numdevs = len(self.root.split(','))
+            cmd = ["lvcreate", "-n", sr_uuid]
+            if numdevs > 1:
+                lowest = -1
+                for dev in self.root.split(','):
+                    stats = lvutil._getPVstats(dev)
+                    if lowest < 0  or stats['freespace'] < lowest:
+                        lowest = stats['freespace']
+                size_mb = (lowest / (1024 * 1024)) * numdevs
+
+                # Add stripe parameter to command
+                cmd += ["-i", str(numdevs), "-I", "2048"]
+            else:
+                stats = lvutil._getVGstats(self.vgname)
+                size_mb = stats['freespace'] / (1024 * 1024)
+            assert(size_mb > 0)
+            cmd += ["-L", str(size_mb), self.vgname]
+            text = util.pread(cmd)
+
+            cmd = ["lvchange", "-ay", self.remotepath]
+            text = util.pread(cmd)
+        except util.CommandException, inst:
+            raise xs_errors.XenError('LVMCreate', \
+                  opterr='lv operation, error %d' % inst.code)
+        except AssertionError:
+            raise xs_errors.XenError('SRNoSpace', \
+                  opterr='Insufficient space in VG %s' % self.vgname)
+
+        try:
+            util.pread2(["mkfs.xfs", self.remotepath])
+        except util.CommandException, inst:
+            raise xs_errors.XenError('LVMFilesystem', \
+                  opterr='mkfs failed error %d' % inst.code)
+
+        #Update serial number string
+        scsiutil.add_serial_record(self.session, self.sr_ref, \
+                  scsiutil.devlist_to_serialstring(self.root.split(',')))
+
+    def vdi(self, uuid, loadLocked=False):
+        return XFSFileVDI(self, uuid)
+
+    @staticmethod
+    def _is_xfs_available():
+        import distutils.spawn
+        return distutils.spawn.find_executable('mkfs.xfs')
+
+
+class XFSFileVDI(FileSR.FileVDI):
+    def attach(self, sr_uuid, vdi_uuid):
+        if not hasattr(self,'xenstore_data'):
+            self.xenstore_data = {}
+
+        self.xenstore_data['storage-type'] = XFSSR.DRIVER_TYPE
+
+        return super(XFSFileVDI, self).attach(sr_uuid, vdi_uuid)
+
+
+if __name__ == '__main__':
+    SRCommand.run(XFSSR, DRIVER_INFO)
+else:
+    SR.registerSR(XFSSR)
diff --git a/drivers/cleanup.py b/drivers/cleanup.py
index 97c332c..ad1ee86 100755
--- a/drivers/cleanup.py
+++ b/drivers/cleanup.py
@@ -2765,7 +2765,10 @@ def normalizeType(type):
     if type in ["lvm", "lvmoiscsi", "lvmohba", "lvmofcoe"]:
         # temporary while LVHD is symlinked as LVM
         type = SR.TYPE_LVHD
-    if type in ["ext", "nfs", "ocfsoiscsi", "ocfsohba", "smb"]:
+    if type in [
+        "ext", "nfs", "ocfsoiscsi", "ocfsohba", "smb", "cephfs", "glusterfs",
+        "xfs"
+    ]:
         type = SR.TYPE_FILE
     if not type in SR.TYPES:
         raise util.SMException("Unsupported SR type: %s" % type)
-- 
2.41.0

